{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eafda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the environment with necessary imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import copy\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Mount drive (if using Google Colab)\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive/\")\n",
    "DATA_DIRECTORY = \"/content/drive/MyDrive/psych254a_2025/data\"\n",
    "\n",
    "# set random seed\n",
    "SEED = 111\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "check_random_state(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aa04d4",
   "metadata": {},
   "source": [
    "# Mixed Effects Models in Behavioral Science\n",
    "\n",
    "In this lecture, we will explore mixed effects models using PyTorch to analyze word acquisition data:\n",
    "\n",
    "1. **Introduction to Mixed Effects Models**: Understanding when and why to use them\n",
    "2. **Logistic Growth Models**: Implementing sigmoid functions for developmental trajectories\n",
    "3. **Random Effects Logistic Models**: Adding word-specific parameters\n",
    "4. **Model Comparison**: Evaluating the benefit of random effects\n",
    "\n",
    "We'll build on your existing knowledge of PyTorch models while demonstrating the flexibility it offers for implementing custom statistical models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2940a9",
   "metadata": {},
   "source": [
    "## 1. Introduction to Mixed Effects Models\n",
    "\n",
    "Mixed effects models (also called multilevel or hierarchical models) are designed for data with nested or grouped structure. They include both fixed effects (population-level parameters) and random effects (group-specific parameters).\n",
    "\n",
    "The standard linear mixed model can be written as:\n",
    "\n",
    "$$y_{ij} = \\beta_0 + \\beta_1 x_{ij} + \\ldots + \\beta_p x_{ij}^{(p)} + b_{0i} + b_{1i} x_{ij} + \\ldots + b_{qi} x_{ij}^{(q)} + \\epsilon_{ij}$$\n",
    "\n",
    "Where:\n",
    "- $y_{ij}$ is the response for observation $j$ in group $i$\n",
    "- $\\beta_0, \\beta_1, \\ldots, \\beta_p$ are fixed effects (population parameters)\n",
    "- $b_{0i}, b_{1i}, \\ldots, b_{qi}$ are random effects for group $i$ (deviations from population parameters)\n",
    "- $\\epsilon_{ij}$ is the error term\n",
    "\n",
    "The random effects are typically assumed to follow a multivariate normal distribution:\n",
    "\n",
    "$$\\mathbf{b}_i \\sim {N}(\\mathbf{0}, \\mathbf{\\Sigma})$$\n",
    "\n",
    "Where $\\mathbf{\\Sigma}$ is the variance-covariance matrix of the random effects.\n",
    "\n",
    "In this lecture, we'll extend this framework to a logistic growth model for word acquisition data. Logistic models are appropriate for developmental data where there is an S-shaped growth curve from 0% to 100%.\n",
    "\n",
    "The standard logistic function is:\n",
    "\n",
    "$$f(x) = \\frac{L}{1 + e^{-k(x-x_0)}}$$\n",
    "\n",
    "Where:\n",
    "- $L$ is the maximum value (usually 1 for proportions)\n",
    "- $k$ is the growth rate (steepness of the curve)\n",
    "- $x_0$ is the midpoint (age at which 50% of children produce the word)\n",
    "\n",
    "By implementing this in PyTorch, we gain tremendous flexibility to customize our models while leveraging automatic differentiation for parameter estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606efb1a",
   "metadata": {},
   "source": [
    "## 2. Data Preparation and Exploration\n",
    "\n",
    "First, let's load and explore the Wordbank data, which contains the percentage of children producing various words at different ages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92613a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wordbank_word data\n",
    "wordbank_path = os.path.join(DATA_DIRECTORY, \"wordbank_item_data.csv\")\n",
    "wordbank_data = pd.read_csv(wordbank_path)\n",
    "\n",
    "# let's take 50 random words\n",
    "wordbank_data = wordbank_data.sample(n=50, random_state=SEED)\n",
    "\n",
    "# Explore the data\n",
    "print(f\"Shape of data: {wordbank_data.shape}\")\n",
    "print(f\"Column names: {wordbank_data.columns[:10]}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "wordbank_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8c80e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data needs some cleaning and restructuring for modeling\n",
    "# First, let's identify the age columns (those that start with numbers)\n",
    "age_columns = [col for col in wordbank_data.columns if col[0].isdigit()]\n",
    "print(f\"Age columns: {age_columns}\")\n",
    "\n",
    "# Create a long-format dataframe with word, age, and production percentage\n",
    "data_long = pd.melt(\n",
    "    wordbank_data,\n",
    "    id_vars=['item_definition', 'item_id'],\n",
    "    value_vars=age_columns,\n",
    "    var_name='age',\n",
    "    value_name='production_pct'\n",
    ")\n",
    "\n",
    "# Convert age from string to numeric (remove 'mo' suffix)\n",
    "data_long['age'] = data_long['age'].str.replace('mo', '').astype(int)\n",
    "\n",
    "# Convert percentage to proportion (0-1 scale)\n",
    "data_long['production_prop'] = data_long['production_pct'] / 100\n",
    "\n",
    "# Filter out extreme values (optional)\n",
    "data_long = data_long[(data_long['production_prop'] >= 0) & (data_long['production_prop'] <= 1)]\n",
    "\n",
    "print(f\"Shape of long-format data: {data_long.shape}\")\n",
    "print(\"\\nFirst few rows of long-format data:\")\n",
    "data_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708adbc1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Explore words and their acquisition curves\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Select a few common words to visualize\n",
    "random_word_indices = np.random.choice(len(data_long['item_definition'].unique()), 10, replace=False)\n",
    "demonstration_words = data_long['item_definition'].unique()[random_word_indices]\n",
    "for word in demonstration_words:\n",
    "    word_data = data_long[data_long['item_definition'] == word]\n",
    "    plt.plot(word_data['age'], word_data['production_prop'], 'o-', label=word)\n",
    "\n",
    "plt.title(\"Word Acquisition Curves for Common Words\")\n",
    "plt.xlabel(\"Age (months)\")\n",
    "plt.ylabel(\"Proportion of Children Producing Word\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204ce7b5",
   "metadata": {},
   "source": [
    "As we can see from the exploration, different words follow different acquisition trajectories. Some words are learned earlier than others, and the rate of acquisition varies across words. This suggests that a mixed effects model with word-specific parameters would be appropriate.\n",
    "\n",
    "## 3. Implementing a Logistic Growth Model\n",
    "\n",
    "Let's start by implementing a simple logistic growth model with fixed effects only. This will serve as our baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae197d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class UniversalProcedure:\n",
    "    \"\"\"A class to implement the universal procedure for model training and evaluation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, cross_validator=None, evaluation_metrics={}, loss_func=None, optimizer=None\n",
    "    ):\n",
    "\n",
    "        self.cross_validator = cross_validator\n",
    "\n",
    "        if not evaluation_metrics:\n",
    "            self.evaluation_metrics = {\n",
    "                'MSE': nn.MSELoss(),\n",
    "                'R^2': lambda y, y_pred: 1 - torch.sum((y - y_pred)**2) / torch.sum((y - torch.mean(y))**2)\n",
    "            }\n",
    "        else:\n",
    "            self.evaluation_metrics = evaluation_metrics\n",
    "\n",
    "        if loss_func is None:\n",
    "            self.loss_func = nn.MSELoss()\n",
    "        else:\n",
    "            self.loss_func = loss_func\n",
    "\n",
    "        if optimizer is None:\n",
    "            self.optimizer = optim.Adam\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        model,  # The model to train\n",
    "        X_train,  # Features\n",
    "        y_train,  # Target values\n",
    "        word_ids=None,  # Word IDs (for random effects)\n",
    "        train_epochs=1000,  # Increase epochs\n",
    "        lr=0.01,  # Learning rate\n",
    "        reg_lambda=0.01  # Regularization strength\n",
    "    ):\n",
    "        # Initialize optimizer\n",
    "        optimizer = self.optimizer(model.parameters(), lr=lr)\n",
    "        loss_fn = self.loss_func\n",
    "        \n",
    "        # For tracking progress\n",
    "        losses = []\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in tqdm(range(train_epochs)):\n",
    "            # Forward pass\n",
    "            if word_ids is not None:\n",
    "                y_pred = model(X_train, word_ids)\n",
    "            else:\n",
    "                y_pred = model(X_train)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(y_pred, y_train)\n",
    "            \n",
    "            # Add regularization for random effects\n",
    "            if hasattr(model, 'random_x0'):\n",
    "                loss = loss + reg_lambda * torch.sum(model.random_x0**2)\n",
    "            if hasattr(model, 'random_k'):\n",
    "                loss = loss + reg_lambda * torch.sum(model.random_k**2)\n",
    "            \n",
    "            # Store loss\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Add early stopping based on loss plateau\n",
    "            if epoch > 100 and epoch % 100 == 0:\n",
    "                if abs(losses[-100] - losses[-1]) < 1e-4:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "        \n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff17f3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class FixedEffectsLogisticModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize a logistic growth model with fixed effects only.\n",
    "        \"\"\"\n",
    "        super(FixedEffectsLogisticModel, self).__init__()\n",
    "\n",
    "        # Fixed effects (population parameters)\n",
    "        self.fixed_L = nn.Parameter(torch.tensor(1.0))  # Maximum value (typically 1 for proportions)\n",
    "        self.fixed_x0 = nn.Parameter(torch.tensor(18.0))  # Midpoint (age at 50% acquisition)\n",
    "        self.fixed_k = nn.Parameter(torch.tensor(0.3))  # Growth rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass implementing the logistic function.\n",
    "\n",
    "        Args:\n",
    "            x: Age values (months)\n",
    "\n",
    "        Returns:\n",
    "            Predicted proportion of children producing words at each age\n",
    "        \"\"\"\n",
    "        # Apply sigmoid function with fixed parameters\n",
    "        return self.fixed_L / (1 + torch.exp(-self.fixed_k * (x - self.fixed_x0)))\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Make predictions (for compatibility with evaluation frameworks).\n",
    "        \"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec3929f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Prepare data for PyTorch modeling\n",
    "# Create word-to-index mapping\n",
    "unique_words = data_long['item_definition'].unique()\n",
    "word_to_idx = {word: i for i, word in enumerate(unique_words)}\n",
    "num_words = len(unique_words)\n",
    "\n",
    "# Convert to tensors\n",
    "X = torch.tensor(data_long['age'].values, dtype=torch.float32).reshape(-1, 1)\n",
    "y = torch.tensor(data_long['production_prop'].values, dtype=torch.float32).reshape(-1, 1)\n",
    "word_ids = torch.tensor([word_to_idx[word] for word in data_long['item_definition']], dtype=torch.long)\n",
    "\n",
    "# Train the fixed effects logistic model\n",
    "fixed_model = FixedEffectsLogisticModel()\n",
    "procedure = UniversalProcedure()\n",
    "fixed_losses = procedure.train(fixed_model, X, y, train_epochs=500, lr=0.05)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fixed_losses)\n",
    "plt.title('Fixed Effects Logistic Model: Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49bcd36",
   "metadata": {},
   "source": [
    "## 4. Random Effects Logistic Models\n",
    "\n",
    "Now, let's implement a mixed effects logistic model. We'll start with random midpoints (x0) only, allowing each word to have its own age of 50% acquisition, but sharing the growth rate parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5468b3a2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class RandomMidpointLogisticModel(nn.Module):\n",
    "    def __init__(self, num_words):\n",
    "        \"\"\"\n",
    "        Initialize a logistic growth model with random midpoints (x0).\n",
    "\n",
    "        Args:\n",
    "            num_words: Number of unique words\n",
    "        \"\"\"\n",
    "        super(RandomMidpointLogisticModel, self).__init__()\n",
    "\n",
    "        # Fixed effects (population parameters)\n",
    "        self.fixed_L = nn.Parameter(torch.tensor(1.0))  # Maximum value\n",
    "        self.fixed_x0 = nn.Parameter(torch.tensor(18.0))  # Population midpoint\n",
    "        self.fixed_k = nn.Parameter(torch.tensor(0.3))  # Growth rate\n",
    "\n",
    "        # Random effects (word-specific deviations)\n",
    "        # Initialize with small random values instead of zeros\n",
    "        self.random_x0 = nn.Parameter(torch.randn(num_words) * 0.1)  # Word-specific deviations from population midpoint\n",
    "\n",
    "    def forward(self, x, word_ids):\n",
    "        \"\"\"\n",
    "        Forward pass implementing the logistic function with random midpoints.\n",
    "\n",
    "        Args:\n",
    "            x: Age values (months)\n",
    "            word_ids: Word indices for each observation\n",
    "\n",
    "        Returns:\n",
    "            Predicted proportion of children producing words at each age\n",
    "        \"\"\"\n",
    "        # Get word-specific midpoints (fixed + random)\n",
    "        word_x0 = self.fixed_x0 + self.random_x0[word_ids]\n",
    "\n",
    "        # Apply sigmoid function with word-specific midpoints\n",
    "        # Add a small epsilon for numerical stability\n",
    "        return self.fixed_L / (1 + torch.exp(-self.fixed_k * (x - word_x0)))\n",
    "\n",
    "    def predict(self, x, word_ids):\n",
    "        \"\"\"\n",
    "        Make predictions (for compatibility with evaluation frameworks).\n",
    "        \"\"\"\n",
    "        return self.forward(x, word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f5842",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Train the random midpoint logistic model\n",
    "random_x0_model = RandomMidpointLogisticModel(num_words)\n",
    "random_x0_losses = procedure.train(random_x0_model, X, y, word_ids, train_epochs=500, lr=0.05)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(random_x0_losses)\n",
    "plt.title('Random Midpoint Logistic Model: Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8085e3",
   "metadata": {},
   "source": [
    "Now, let's implement a full mixed effects logistic model with both random midpoints (x0) and random growth rates (k). This allows each word to have its own age of 50% acquisition and its own learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b957f9a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class FullRandomEffectsLogisticModel(nn.Module):\n",
    "    def __init__(self, num_words):\n",
    "        \"\"\"\n",
    "        Initialize a logistic growth model with random midpoints (x0) and random growth rates (k).\n",
    "\n",
    "        Args:\n",
    "            num_words: Number of unique words\n",
    "        \"\"\"\n",
    "        super(FullRandomEffectsLogisticModel, self).__init__()\n",
    "\n",
    "        # Fixed effects (population parameters)\n",
    "        self.fixed_L = nn.Parameter(torch.tensor(1.0))  # Maximum value\n",
    "        self.fixed_x0 = nn.Parameter(torch.tensor(18.0))  # Population midpoint\n",
    "        self.fixed_k = nn.Parameter(torch.tensor(0.3))  # Growth rate\n",
    "\n",
    "        # Random effects (word-specific deviations)\n",
    "        # Initialize with small random values\n",
    "        self.random_x0 = nn.Parameter(torch.randn(num_words) * 0.1)  # Word-specific deviations from population midpoint\n",
    "        self.random_k = nn.Parameter(torch.randn(num_words) * 0.01)  # Word-specific deviations from population growth rate\n",
    "\n",
    "    def forward(self, x, word_ids):\n",
    "        \"\"\"\n",
    "        Forward pass implementing the logistic function with random midpoints and growth rates.\n",
    "\n",
    "        Args:\n",
    "            x: Age values (months)\n",
    "            word_ids: Word indices for each observation\n",
    "\n",
    "        Returns:\n",
    "            Predicted proportion of children producing words at each age\n",
    "        \"\"\"\n",
    "        # Get word-specific parameters (fixed + random)\n",
    "        word_x0 = self.fixed_x0 + self.random_x0[word_ids]\n",
    "        word_k = self.fixed_k + self.random_k[word_ids]\n",
    "        \n",
    "        # Ensure k is positive by using softplus or abs\n",
    "        word_k = torch.abs(word_k)  # Or use F.softplus(word_k)\n",
    "        \n",
    "        # Apply sigmoid function with word-specific parameters\n",
    "        return self.fixed_L / (1 + torch.exp(-word_k * (x - word_x0)))\n",
    "\n",
    "    def predict(self, x, word_ids):\n",
    "        \"\"\"\n",
    "        Make predictions (for compatibility with evaluation frameworks).\n",
    "        \"\"\"\n",
    "        return self.forward(x, word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0bb194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the full random effects logistic model\n",
    "full_random_model = FullRandomEffectsLogisticModel(num_words)\n",
    "full_random_losses = procedure.train(full_random_model, X, y, word_ids, train_epochs=500, lr=0.05)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(full_random_losses)\n",
    "plt.title('Full Random Effects Logistic Model: Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0eb9d1",
   "metadata": {},
   "source": [
    "## 5. Model Comparison and Visualization\n",
    "\n",
    "Let's compare the performance of our models and visualize the word-specific acquisition curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4957840",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "with torch.no_grad():\n",
    "    # Fixed effects model\n",
    "    y_pred_fixed = fixed_model(X)\n",
    "    mse_fixed = nn.MSELoss()(y_pred_fixed, y)\n",
    "    r2_fixed = 1 - torch.sum((y - y_pred_fixed)**2) / torch.sum((y - torch.mean(y))**2)\n",
    "\n",
    "    # Random midpoint model\n",
    "    y_pred_x0 = random_x0_model(X, word_ids)\n",
    "    mse_x0 = nn.MSELoss()(y_pred_x0, y)\n",
    "    r2_x0 = 1 - torch.sum((y - y_pred_x0)**2) / torch.sum((y - torch.mean(y))**2)\n",
    "\n",
    "    # Full random effects model\n",
    "    y_pred_full = full_random_model(X, word_ids)\n",
    "    mse_full = nn.MSELoss()(y_pred_full, y)\n",
    "    r2_full = 1 - torch.sum((y - y_pred_full)**2) / torch.sum((y - torch.mean(y))**2)\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(f\"Fixed Effects Model:       MSE = {mse_fixed.item():.4f}, R² = {r2_fixed.item():.4f}\")\n",
    "print(f\"Random Midpoint Model:     MSE = {mse_x0.item():.4f}, R² = {r2_x0.item():.4f}\")\n",
    "print(f\"Full Random Effects Model: MSE = {mse_full.item():.4f}, R² = {r2_full.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b015282",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Visualize fixed model predictions for selected words\n",
    "def plot_word_predictions(word_list, model, is_random_effects=False):\n",
    "    \"\"\"\n",
    "    Plot the actual data and model predictions for a list of words.\n",
    "    \n",
    "    Args:\n",
    "        word_list: List of words to visualize\n",
    "        model: The trained model\n",
    "        is_random_effects: Whether the model has random effects (needs word IDs)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    age_range = torch.linspace(8, 30, 100).reshape(-1, 1)\n",
    "    \n",
    "    for i, word in enumerate(word_list):\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        \n",
    "        # Get actual data for this word\n",
    "        word_data = data_long[data_long['item_definition'] == word]\n",
    "        plt.scatter(word_data['age'], word_data['production_prop'], color='blue', \n",
    "                   alpha=0.7, label='Observed Data')\n",
    "        \n",
    "        # Get model predictions\n",
    "        if is_random_effects:\n",
    "            word_idx = word_to_idx[word]\n",
    "            word_id_tensor = torch.tensor([word_idx] * len(age_range))\n",
    "            with torch.no_grad():\n",
    "                predictions = model(age_range, word_id_tensor)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                predictions = model(age_range)\n",
    "        \n",
    "        # Plot model predictions\n",
    "        plt.plot(age_range, predictions, 'r-', linewidth=2, label='Model Prediction')\n",
    "        \n",
    "        # Add titles and labels\n",
    "        plt.title(f'\"{word}\"')\n",
    "        plt.xlabel('Age (months)')\n",
    "        plt.ylabel('Proportion')\n",
    "        plt.ylim(-0.05, 1.05)\n",
    "        \n",
    "        # Only add legend to the first subplot\n",
    "        if i == 0:\n",
    "            plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dcb0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trained fixed effects model\n",
    "plot_word_predictions(demonstration_words, fixed_model, is_random_effects=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b5ee76",
   "metadata": {},
   "source": [
    "As we can see, the fixed effects model uses the same curve for all words, which doesn't capture the variation between words. Some words are learned earlier than others, and some have different rates of acquisition. Let's extend our model to allow for word-specific midpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d29e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training the random midpoint model, visualize its predictions\n",
    "plot_word_predictions(demonstration_words, random_x0_model, is_random_effects=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cc2655",
   "metadata": {},
   "source": [
    "The random midpoint model allows each word to have its own midpoint (x0), which better captures when words are learned. However, we can see that for some words, the steepness of the curve still doesn't match the data well. Let's extend to a full random effects model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ceffc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# After training the full random effects model, visualize its predictions\n",
    "plot_word_predictions(demonstration_words, full_random_model, is_random_effects=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ef96e4",
   "metadata": {},
   "source": [
    "The full random effects model, with both word-specific midpoints and growth rates, provides the best fit to the data, capturing both when words are learned and how quickly they spread through the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80265ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three models on the same plot for a few selected words\n",
    "def compare_models(word_list, fixed_model, random_x0_model, full_random_model):\n",
    "    \"\"\"\n",
    "    Compare predictions from all three models for selected words.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    age_range = torch.linspace(8, 30, 100).reshape(-1, 1)\n",
    "    \n",
    "    for i, word in enumerate(word_list):\n",
    "        plt.subplot(3, 2, i+1)\n",
    "        \n",
    "        # Get actual data\n",
    "        word_data = data_long[data_long['item_definition'] == word]\n",
    "        plt.scatter(word_data['age'], word_data['production_prop'], color='black', \n",
    "                   alpha=0.7, label='Observed Data')\n",
    "        \n",
    "        # Get word index for random effects models\n",
    "        word_idx = word_to_idx[word]\n",
    "        word_id_tensor = torch.tensor([word_idx] * len(age_range))\n",
    "        \n",
    "        # Get predictions from all models\n",
    "        with torch.no_grad():\n",
    "            fixed_pred = fixed_model(age_range)\n",
    "            random_x0_pred = random_x0_model(age_range, word_id_tensor)\n",
    "            full_random_pred = full_random_model(age_range, word_id_tensor)\n",
    "        \n",
    "        # Plot predictions\n",
    "        plt.plot(age_range, fixed_pred, 'r-', linewidth=2, label='Fixed Effects')\n",
    "        plt.plot(age_range, random_x0_pred, 'g-', linewidth=2, label='Random x0')\n",
    "        plt.plot(age_range, full_random_pred, 'b-', linewidth=2, label='Random x0 & k')\n",
    "        \n",
    "        plt.title(f'\"{word}\"')\n",
    "        plt.xlabel('Age (months)')\n",
    "        plt.ylabel('Proportion')\n",
    "        plt.ylim(-0.05, 1.05)\n",
    "        \n",
    "        # Only add legend to the first subplot\n",
    "        if i == 0:\n",
    "            plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compare all three models for a subset of words\n",
    "comparison_words = ['mommy', 'water', 'ball', 'computer', 'elephant', 'telephone']\n",
    "compare_models(comparison_words, fixed_model, random_x0_model, full_random_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2742e1",
   "metadata": {},
   "source": [
    "## 6. Semantic Category Exploration\n",
    "\n",
    "Let's explore whether words from the same semantic category have similar acquisition patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac21180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some semantic categories (simplified example)\n",
    "categories = {\n",
    "    'animals': ['dog', 'cat', 'horse', 'cow', 'sheep', 'pig', 'fish', 'bird'],\n",
    "    'food': ['banana', 'apple', 'cookie', 'juice', 'water', 'milk', 'bread', 'cheese'],\n",
    "    'people': ['mommy', 'daddy', 'baby', 'grandma', 'grandpa', 'friend'],\n",
    "    'household': ['phone', 'chair', 'table', 'bath', 'potty', 'blanket', 'bed', 'couch'],\n",
    "    'toys': ['ball', 'book', 'toy', 'doll', 'teddy', 'balloon', 'car', 'block']\n",
    "}\n",
    "\n",
    "# Collect parameter values by category\n",
    "category_data = []\n",
    "with torch.no_grad():\n",
    "    fixed_x0 = full_random_model.fixed_x0.item()\n",
    "    fixed_k = full_random_model.fixed_k.item()\n",
    "    random_x0 = full_random_model.random_x0.detach().numpy()\n",
    "    random_k = full_random_model.random_k.detach().numpy()\n",
    "\n",
    "for category, words in categories.items():\n",
    "    for word in words:\n",
    "        if word in word_to_idx:\n",
    "            idx = word_to_idx[word]\n",
    "            category_data.append({\n",
    "                'word': word,\n",
    "                'category': category,\n",
    "                'x0': fixed_x0 + random_x0[idx],\n",
    "                'k': fixed_k + random_k[idx]\n",
    "            })\n",
    "\n",
    "category_df = pd.DataFrame(category_data)\n",
    "\n",
    "# Visualize parameters by category\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(x='category', y='x0', data=category_df)\n",
    "plt.title('Midpoint (x0) by Semantic Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Age at 50% Acquisition')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='category', y='k', data=category_df)\n",
    "plt.title('Growth Rate (k) by Semantic Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Growth Rate')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c3c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize acquisition curves by category\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, (category, words) in enumerate(categories.items()):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "\n",
    "    # Plot acquisition curves for each word in the category\n",
    "    for word in words:\n",
    "        if word in word_to_idx:\n",
    "            word_idx = word_to_idx[word]\n",
    "            word_id_tensor = torch.tensor([word_idx] * len(age_range))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                word_pred = full_random_model(age_range, word_id_tensor)\n",
    "\n",
    "            plt.plot(age_range, word_pred, alpha=0.7, label=word if i == 0 else None)\n",
    "\n",
    "    plt.title(f'Category: {category}')\n",
    "    plt.xlabel('Age (months)')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.ylim(-0.05, 1.05)\n",
    "\n",
    "    if i == 0:\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef553708",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this lecture, we've explored mixed effects models for logistic growth curves using PyTorch:\n",
    "\n",
    "1. **Fixed Effects Logistic Model**: A baseline model with the same parameters for all words.\n",
    "\n",
    "2. **Random Midpoint Model**: Allowing each word to have its own age of 50% acquisition.\n",
    "\n",
    "3. **Full Random Effects Model**: Extending to word-specific growth rates as well.\n",
    "\n",
    "Our results show that:\n",
    "\n",
    "- Different words follow distinct developmental trajectories\n",
    "- Adding random effects significantly improves model fit\n",
    "- Words within semantic categories show some similarities in acquisition patterns\n",
    "\n",
    "The flexibility of PyTorch enabled us to implement these custom mixed effects models efficiently and extend them beyond what's typically available in standard statistical packages. By leveraging automatic differentiation and custom model architectures, we can create models that are tailored to the specific properties of our developmental data.\n",
    "\n",
    "This approach can be extended to more complex models with multiple predictors, interactions, and additional random effects as needed for specific research questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e444ec",
   "metadata": {},
   "source": [
    "## 8. Additional Exercises (optional)\n",
    "\n",
    "1. **Regularization Exploration**: Experiment with different regularization strengths for the random effects and observe how they affect model fit and parameter estimates.\n",
    "\n",
    "2. **Additional Random Effects**: Extend the model to include a random asymptote (L) parameter for each word.\n",
    "\n",
    "3. **Category-Level Effects**: Implement a three-level model with effects at the word level nested within semantic categories.\n",
    "\n",
    "4. **Model Validation**: Implement cross-validation to assess the generalization performance of the different models."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
