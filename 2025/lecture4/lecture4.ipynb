{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c775de6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Set up the environment with necessary imports\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import networkx as nx\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f91e69",
   "metadata": {},
   "source": [
    "# Computation Graphs and Automatic Differentiation\n",
    "\n",
    "In this interactive lecture, we'll explore the fundamental concepts behind modern deep learning optimization:\n",
    "\n",
    "1. **Computation Graphs**: How PyTorch represents operations\n",
    "2. **Automatic Differentiation**: How PyTorch computes gradients\n",
    "3. **Benefits of Autograd**: Handling complex functions\n",
    "4. **Working with Autodiff**: Practical considerations\n",
    "\n",
    "Each section contains explanations and hands-on exercises to build your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddca668",
   "metadata": {},
   "source": [
    "## 1. Computation Graphs\n",
    "\n",
    "### 1.1 What is a Computation Graph?\n",
    "\n",
    "A computation graph represents mathematical operations as a directed graph:\n",
    "- **Nodes**: Variables (inputs, intermediate values, outputs)\n",
    "- **Edges**: Operations between variables\n",
    "- **Direction**: The flow of computation from inputs to outputs\n",
    "\n",
    "Let's visualize a computation graph for a simple function:\n",
    "$$f(x) = x^2 + 2x + 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7447bc3e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to visualize a computation graph\n",
    "def visualize_computation_graph(nodes, edges, edge_labels=None, node_labels=None):\n",
    "    \"\"\"\n",
    "    Visualize a computation graph\n",
    "    \n",
    "    Parameters:\n",
    "    - nodes: List of node names\n",
    "    - edges: List of tuples (source, target) representing connections\n",
    "    - edge_labels: Dictionary of edge labels {(source, target): operation}\n",
    "    - node_labels: Dictionary of node labels {node: value}\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from(edges)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    pos = nx.spring_layout(G, seed=42)  # positions for all nodes\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=2000, node_color='lightblue')\n",
    "    \n",
    "    # Draw edges with arrows\n",
    "    nx.draw_networkx_edges(G, pos, width=2, arrowsize=20, arrowstyle='->')\n",
    "    \n",
    "    # Draw node labels\n",
    "    if node_labels:\n",
    "        nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=12)\n",
    "    else:\n",
    "        nx.draw_networkx_labels(G, pos, font_size=12)\n",
    "    \n",
    "    # Draw edge labels\n",
    "    if edge_labels:\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=10)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1734cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the computation graph for f(x) = x^2 + 2x + 1\n",
    "x_value = 2.0\n",
    "x = torch.tensor(x_value, requires_grad=True)\n",
    "\n",
    "# Build the graph step by step\n",
    "y1 = x * x           # x^2\n",
    "y2 = 2 * x           # 2x\n",
    "y3 = y1 + y2         # x^2 + 2x\n",
    "y4 = y3 + 1          # x^2 + 2x + 1\n",
    "\n",
    "# Define nodes and edges for visualization\n",
    "nodes = ['x', 'y1', 'y2', 'y3', 'y4']\n",
    "edges = [('x', 'y1'), ('x', 'y2'), ('y1', 'y3'), ('y2', 'y3'), ('y3', 'y4')]\n",
    "\n",
    "# Define edge labels (operations)\n",
    "edge_labels = {\n",
    "    ('x', 'y1'): 'square',\n",
    "    ('x', 'y2'): '* 2',\n",
    "    ('y1', 'y3'): '+',\n",
    "    ('y2', 'y3'): '+',\n",
    "    ('y3', 'y4'): '+ 1'\n",
    "}\n",
    "\n",
    "# Define node labels with values\n",
    "node_labels = {\n",
    "    'x': f'x = {x.item()}',\n",
    "    'y1': f'y1 = x² = {y1.item()}',\n",
    "    'y2': f'y2 = 2x = {y2.item()}',\n",
    "    'y3': f'y3 = x² + 2x = {y3.item()}',\n",
    "    'y4': f'y4 = x² + 2x + 1 = {y4.item()}'\n",
    "}\n",
    "\n",
    "# Visualize the graph\n",
    "visualize_computation_graph(nodes, edges, edge_labels, node_labels)\n",
    "\n",
    "print(f\"Function value at x = {x_value}: {y4.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd27fdd",
   "metadata": {},
   "source": [
    "### 1.2 Exercise: Build Your Own Computation Graph\n",
    "\n",
    "Now it's your turn to build a computation graph for the function:\n",
    "$$g(x) = 3x^2 - 4x + 2$$\n",
    "\n",
    "1. Create the variables and operations needed\n",
    "2. Define the nodes, edges, and labels for visualization\n",
    "3. Visualize the graph using the helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f8d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50e53c3",
   "metadata": {},
   "source": [
    "### Solution 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3329d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the computation graph for g(x) = 3x^2 - 4x + 2\n",
    "x_value = 2.0\n",
    "x = torch.tensor(x_value, requires_grad=True)\n",
    "\n",
    "# Build the graph step by step\n",
    "x_squared = x * x        # x^2\n",
    "term1 = 3 * x_squared    # 3x^2\n",
    "term2 = 4 * x            # 4x\n",
    "term3 = term1 - term2    # 3x^2 - 4x\n",
    "g = term3 + 2            # 3x^2 - 4x + 2\n",
    "\n",
    "# Define nodes and edges for visualization\n",
    "nodes = ['x', 'x_squared', 'term1', 'term2', 'term3', 'g']\n",
    "edges = [\n",
    "    ('x', 'x_squared'), \n",
    "    ('x_squared', 'term1'), \n",
    "    ('x', 'term2'), \n",
    "    ('term1', 'term3'), \n",
    "    ('term2', 'term3'), \n",
    "    ('term3', 'g')\n",
    "]\n",
    "\n",
    "# Define edge labels\n",
    "edge_labels = {\n",
    "    ('x', 'x_squared'): 'square',\n",
    "    ('x_squared', 'term1'): '* 3',\n",
    "    ('x', 'term2'): '* 4',\n",
    "    ('term1', 'term3'): '-',\n",
    "    ('term2', 'term3'): '-',\n",
    "    ('term3', 'g'): '+ 2'\n",
    "}\n",
    "\n",
    "# Define node labels with values\n",
    "node_labels = {\n",
    "    'x': f'x = {x.item()}',\n",
    "    'x_squared': f'x² = {x_squared.item()}',\n",
    "    'term1': f'3x² = {term1.item()}',\n",
    "    'term2': f'4x = {term2.item()}',\n",
    "    'term3': f'3x² - 4x = {term3.item()}',\n",
    "    'g': f'g = 3x² - 4x + 2 = {g.item()}'\n",
    "}\n",
    "\n",
    "# Visualize the graph\n",
    "visualize_computation_graph(nodes, edges, edge_labels, node_labels)\n",
    "\n",
    "print(f\"Function value at x = {x_value}: {g.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb8761f",
   "metadata": {},
   "source": [
    "## 2. Automatic Differentiation\n",
    "\n",
    "### 2.1 Understanding Autograd\n",
    "\n",
    "PyTorch's automatic differentiation (autograd) allows us to compute derivatives automatically. This works by:\n",
    "\n",
    "1. **Building a dynamic computation graph** as operations are performed\n",
    "2. **Recording operations** on variables that have `requires_grad=True`\n",
    "3. **Applying the chain rule** to compute gradients during backward pass\n",
    "\n",
    "The **chain rule** is a fundamental concept in calculus that allows us to compute derivatives of composite functions:\n",
    "\n",
    "For a composite function $f(g(x))$, the derivative is:\n",
    "$$\\frac{d}{dx}f(g(x)) = \\frac{df}{dg} \\cdot \\frac{dg}{dx}$$\n",
    "\n",
    "For example, if $y = f(x) = x^2 + 2x + 1$, we can break it down:\n",
    "- $y_1 = x^2$, so $\\frac{dy_1}{dx} = 2x$\n",
    "- $y_2 = 2x$, so $\\frac{dy_2}{dx} = 2$\n",
    "- $y_3 = y_1 + y_2$, so $\\frac{dy_3}{dy_1} = 1$ and $\\frac{dy_3}{dy_2} = 1$\n",
    "- $y_4 = y_3 + 1$, so $\\frac{dy_4}{dy_3} = 1$\n",
    "\n",
    "Using the chain rule:\n",
    "$$\\frac{dy_4}{dx} = \\frac{dy_4}{dy_3} \\cdot \\left(\\frac{dy_3}{dy_1} \\cdot \\frac{dy_1}{dx} + \\frac{dy_3}{dy_2} \\cdot \\frac{dy_2}{dx}\\right) = 1 \\cdot (1 \\cdot 2x + 1 \\cdot 2) = 2x + 2$$\n",
    "\n",
    "PyTorch automates this process by traversing the computation graph backward from the output to the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a63448",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Let's compute the gradient of our function f(x) = x^2 + 2x + 1\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**2 + 2*x + 1\n",
    "\n",
    "# Compute the gradient\n",
    "y.backward()\n",
    "\n",
    "# Access the gradient\n",
    "print(f\"Function: f(x) = x^2 + 2x + 1\")\n",
    "print(f\"Value at x = 2: {y.item()}\")\n",
    "print(f\"Gradient at x = 2: {x.grad.item()}\")\n",
    "print(f\"Expected gradient (2x + 2): {2*2 + 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d9aa5d",
   "metadata": {},
   "source": [
    "### 2.2 Gradients in Multiple Dimensions\n",
    "\n",
    "For functions with multiple inputs, autograd computes partial derivatives with respect to each input.\n",
    "\n",
    "Let's look at a 2D function: $f(x, y) = x^2 + y^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdd7e80",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create a function to visualize 2D functions and their gradients\n",
    "def visualize_function_and_gradient_2d(f, x_range, y_range, point=None, num_points=20):\n",
    "    \"\"\"\n",
    "    Visualize a 2D function and its gradient\n",
    "    \n",
    "    Parameters:\n",
    "    - f: Function that takes x, y and returns z\n",
    "    - x_range: Tuple (x_min, x_max)\n",
    "    - y_range: Tuple (y_min, y_max)\n",
    "    - point: Optional tuple (x0, y0, z0) to mark a specific point\n",
    "    - num_points: Number of points in each dimension for the grid\n",
    "    \"\"\"\n",
    "    # Create a grid of points\n",
    "    x_vals = np.linspace(x_range[0], x_range[1], num_points)\n",
    "    y_vals = np.linspace(y_range[0], y_range[1], num_points)\n",
    "    X, Y = np.meshgrid(x_vals, y_vals)\n",
    "    \n",
    "    # Compute function values\n",
    "    Z = np.zeros_like(X)\n",
    "    grad_x = np.zeros_like(X)\n",
    "    grad_y = np.zeros_like(X)\n",
    "    \n",
    "    # Compute function values and gradients using autograd\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            x_tensor = torch.tensor(X[i, j], requires_grad=True)\n",
    "            y_tensor = torch.tensor(Y[i, j], requires_grad=True)\n",
    "            \n",
    "            # Compute function value\n",
    "            z = f(x_tensor, y_tensor)\n",
    "            Z[i, j] = z.item()\n",
    "            \n",
    "            # Compute gradients\n",
    "            z.backward()\n",
    "            grad_x[i, j] = x_tensor.grad.item()\n",
    "            grad_y[i, j] = y_tensor.grad.item()\n",
    "            \n",
    "            # Reset gradients for next iteration\n",
    "            x_tensor.grad.zero_()\n",
    "            y_tensor.grad.zero_()\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 3D surface plot\n",
    "    ax1 = fig.add_subplot(1, 3, 1, projection='3d')\n",
    "    surf = ax1.plot_surface(X, Y, Z, cmap=cm.coolwarm, alpha=0.8)\n",
    "    ax1.set_xlabel('$x$')\n",
    "    ax1.set_ylabel('$y$')\n",
    "    ax1.set_zlabel('$f(x,y)$')\n",
    "    ax1.set_title('3D Surface Plot')\n",
    "    \n",
    "    # Mark the specific point if provided\n",
    "    if point:\n",
    "        x0, y0, z0 = point\n",
    "        ax1.scatter([x0], [y0], [z0], color='red', s=50)\n",
    "    \n",
    "    # Contour plot with gradient field\n",
    "    ax2 = fig.add_subplot(1, 3, 2)\n",
    "    contour = ax2.contourf(X, Y, Z, 20, cmap=cm.coolwarm)\n",
    "    ax2.quiver(X, Y, grad_x, grad_y, color='black', scale=50)\n",
    "    ax2.set_xlabel('$x$')\n",
    "    ax2.set_ylabel('$y$')\n",
    "    ax2.set_title('Contour Plot with Gradient Field')\n",
    "    plt.colorbar(contour, ax=ax2)\n",
    "    \n",
    "    # Mark the specific point if provided\n",
    "    if point:\n",
    "        x0, y0, _ = point\n",
    "        ax2.plot(x0, y0, 'ro', markersize=10)\n",
    "        # Get the gradient at the specific point\n",
    "        x_tensor = torch.tensor(x0, requires_grad=True)\n",
    "        y_tensor = torch.tensor(y0, requires_grad=True)\n",
    "        z = f(x_tensor, y_tensor)\n",
    "        z.backward()\n",
    "        grad_x0 = x_tensor.grad.item()\n",
    "        grad_y0 = y_tensor.grad.item()\n",
    "        ax2.quiver(x0, y0, grad_x0, grad_y0, color='red', scale=10)\n",
    "        ax2.text(x0+0.1, y0+0.1, f'∇f({x0},{y0}) = [{grad_x0:.2f}, {grad_y0:.2f}]', fontsize=10)\n",
    "    \n",
    "    # Gradient magnitude plot\n",
    "    ax3 = fig.add_subplot(1, 3, 3)\n",
    "    grad_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "    contour2 = ax3.contourf(X, Y, grad_magnitude, 20, cmap='viridis')\n",
    "    ax3.set_xlabel('$x$')\n",
    "    ax3.set_ylabel('$y$')\n",
    "    ax3.set_title('Gradient Magnitude')\n",
    "    plt.colorbar(contour2, ax=ax3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445cf1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple 2D function\n",
    "def f_2d(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "# Point to highlight\n",
    "x0, y0 = 2.0, 3.0\n",
    "z0 = f_2d(x0, y0)\n",
    "\n",
    "# Visualize\n",
    "visualize_function_and_gradient_2d(f_2d, (-4, 4), (-4, 4), point=(x0, y0, z0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e217ec63",
   "metadata": {},
   "source": [
    "### 2.3 Exercise: Compute and Visualize Gradients\n",
    "\n",
    "Now it's your turn! For the function $h(x, y) = x^2 - y^2$:\n",
    "\n",
    "1. Define the function\n",
    "2. Compute the gradient at the point (1, 2) using PyTorch's autograd\n",
    "3. Visualize the function and its gradient using our helper function\n",
    "4. Compare the autograd result with the analytical gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a42b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d308fe67",
   "metadata": {},
   "source": [
    "### Solution 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a372f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 1. Define the function\n",
    "h = lambda x, y: x**2 - y**2\n",
    "\n",
    "# 2. Compute the gradient at (1, 2) using PyTorch's autograd\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "z = x**2 - y**2\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(f\"Function value h(1, 2) = {z.item()}\")\n",
    "print(f\"∂h/∂x at (1, 2) using autograd: {x.grad.item()}\")\n",
    "print(f\"∂h/∂y at (1, 2) using autograd: {y.grad.item()}\")\n",
    "\n",
    "# Analytical gradient\n",
    "print(f\"Analytical ∂h/∂x at (1, 2): {2*1}\")\n",
    "print(f\"Analytical ∂h/∂y at (1, 2): {-2*2}\")\n",
    "\n",
    "# 3. Visualize the function and its gradient\n",
    "x0, y0 = 1.0, 2.0\n",
    "z0 = h(x0, y0)\n",
    "\n",
    "visualize_function_and_gradient_2d(h, (-3, 3), (-3, 3), point=(x0, y0, z0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de15ffa",
   "metadata": {},
   "source": [
    "## 3. The Benefit of Autograd: Computing Gradients for Complex Functions\n",
    "\n",
    "Autograd's real power is its ability to handle complex functions where calculating derivatives by hand would be difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88a4502",
   "metadata": {},
   "source": [
    "### 3.1 Example: Complex Function\n",
    "\n",
    "Let's look at a more complex function: $f(x, y) = \\sin(x^2) \\cdot \\cos(y) + e^{x \\cdot y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9415870f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define the complex function\n",
    "def f_complex(x, y):\n",
    "    return torch.sin(x**2) * torch.cos(y) + torch.exp(x * y)\n",
    "\n",
    "# Compute gradients at (1, 0.5)\n",
    "x0, y0 = 1.0, 0.5\n",
    "z0 = f_complex(torch.tensor(x0), torch.tensor(y0)).item()\n",
    "\n",
    "# Visualize\n",
    "visualize_function_and_gradient_2d(f_complex, (-2, 2), (-2, 2), point=(x0, y0, z0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d6bce",
   "metadata": {},
   "source": [
    "### 3.2 Exercise: Your Complex Function\n",
    "\n",
    "Now it's your turn to experiment with autograd on a function of your choosing!\n",
    "\n",
    "1. Create your own complex function (you can check its 3D shape at https://www.desmos.com/3d)\n",
    "2. Compute its gradient at a point of your choice using PyTorch's autograd\n",
    "3. Visualize your function and its gradient\n",
    "\n",
    "Be creative! Try combinations of trigonometric functions, exponentials, polynomials, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3eb55b",
   "metadata": {},
   "source": [
    "### Solution 3.2 (Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7896295a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# 1. Create a complex function\n",
    "# Let's try: f(x, y) = sin(x*y) * cos(x-y) + log(1 + x^2 + y^2)\n",
    "\n",
    "# Define the function\n",
    "def my_complex_f(x, y):\n",
    "    return torch.sin(x*y) * torch.cos(x-y) + torch.log(1 + x**2 + y**2)\n",
    "\n",
    "# Choose a point\n",
    "x0, y0 = 0.8, 1.2\n",
    "z0 = my_complex_f(torch.tensor(x0), torch.tensor(y0)).item()\n",
    "\n",
    "# Compute gradients and visualize\n",
    "visualize_function_and_gradient_2d(my_complex_f, (-2, 2), (-2, 2), point=(x0, y0, z0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f00f49",
   "metadata": {},
   "source": [
    "## 4. Working with Autodiff\n",
    "\n",
    "### 4.1 Computing Gradients for Different Inputs\n",
    "\n",
    "In deep learning, we often use the same function with different inputs. Let's see how to compute gradients\n",
    "for the same function at different input points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283b44fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: f(x, y) = x^2 * y + y^3\n",
    "\n",
    "def f_demo(x, y):\n",
    "    return x**2 * y + y**3\n",
    "\n",
    "# First point: (2, 1)\n",
    "x1 = torch.tensor(2.0, requires_grad=True)\n",
    "y1 = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "f1 = f_demo(x1, y1)\n",
    "f1.backward()\n",
    "\n",
    "print(\"Point (2, 1):\")\n",
    "print(f\"Function value: {f1.item()}\")\n",
    "print(f\"∂f/∂x: {x1.grad.item()}\")\n",
    "print(f\"∂f/∂y: {y1.grad.item()}\")\n",
    "\n",
    "# To compute gradients at a different point, we need to zero out previous gradients\n",
    "x1.grad.zero_()\n",
    "y1.grad.zero_()\n",
    "\n",
    "# Alternative: create new tensors for the second point\n",
    "x2 = torch.tensor(1.0, requires_grad=True)\n",
    "y2 = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "f2 = f_demo(x2, y2)\n",
    "f2.backward()\n",
    "\n",
    "print(\"\\nPoint (1, 2):\")\n",
    "print(f\"Function value: {f2.item()}\")\n",
    "print(f\"∂f/∂x: {x2.grad.item()}\")\n",
    "print(f\"∂f/∂y: {y2.grad.item()}\")\n",
    "\n",
    "# Visualize both points on the same graph\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create the contour plot\n",
    "x_vals = np.linspace(-2, 3, 30)\n",
    "y_vals = np.linspace(-2, 3, 30)\n",
    "X, Y = np.meshgrid(x_vals, y_vals)\n",
    "\n",
    "# Compute function values\n",
    "Z = np.zeros_like(X)\n",
    "grad_x = np.zeros_like(X)\n",
    "grad_y = np.zeros_like(X)\n",
    "\n",
    "# Compute function values and gradients using autograd\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        x_tensor = torch.tensor(X[i, j], requires_grad=True)\n",
    "        y_tensor = torch.tensor(Y[i, j], requires_grad=True)\n",
    "        \n",
    "        # Compute function value\n",
    "        z = f_demo(x_tensor, y_tensor)\n",
    "        Z[i, j] = z.item()\n",
    "        \n",
    "        # Compute gradients\n",
    "        z.backward()\n",
    "        grad_x[i, j] = x_tensor.grad.item()\n",
    "        grad_y[i, j] = y_tensor.grad.item()\n",
    "        \n",
    "        # Reset gradients for next iteration\n",
    "        x_tensor.grad.zero_()\n",
    "        y_tensor.grad.zero_()\n",
    "\n",
    "# Plot\n",
    "contour = plt.contourf(X, Y, Z, 20, cmap=cm.coolwarm)\n",
    "plt.quiver(X, Y, grad_x, grad_y, color='black', scale=300)\n",
    "\n",
    "# Mark points and their gradients\n",
    "plt.plot(2, 1, 'ro', markersize=10)\n",
    "# Compute gradients at (2, 1)\n",
    "x_tensor = torch.tensor(2.0, requires_grad=True)\n",
    "y_tensor = torch.tensor(1.0, requires_grad=True)\n",
    "z = f_demo(x_tensor, y_tensor)\n",
    "z.backward()\n",
    "gx1, gy1 = x_tensor.grad.item(), y_tensor.grad.item()\n",
    "plt.quiver(2, 1, gx1, gy1, color='red', scale=50)\n",
    "plt.text(2.1, 1.1, f'∇f(2,1) = [{gx1}, {gy1}]', fontsize=10)\n",
    "\n",
    "plt.plot(1, 2, 'go', markersize=10)\n",
    "# Compute gradients at (1, 2)\n",
    "x_tensor = torch.tensor(1.0, requires_grad=True)\n",
    "y_tensor = torch.tensor(2.0, requires_grad=True)\n",
    "z = f_demo(x_tensor, y_tensor)\n",
    "z.backward()\n",
    "gx2, gy2 = x_tensor.grad.item(), y_tensor.grad.item()\n",
    "plt.quiver(1, 2, gx2, gy2, color='green', scale=50)\n",
    "plt.text(1.1, 2.1, f'∇f(1,2) = [{gx2}, {gy2}]', fontsize=10)\n",
    "\n",
    "plt.colorbar(contour)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('Function $f(x,y) = x^2y + y^3$ with Gradients at Different Points')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1218aba5",
   "metadata": {},
   "source": [
    "### 4.2 Using requires_grad=False\n",
    "\n",
    "Sometimes, we don't need to compute gradients for all inputs. We can control this with `requires_grad=False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f3aab0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create tensors with different requires_grad settings\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=False)  # No gradients for y\n",
    "\n",
    "# Compute function\n",
    "f = x**2 * y\n",
    "\n",
    "# Backpropagate\n",
    "f.backward()\n",
    "\n",
    "print(f\"Function f = x^2 * y at (2, 3): {f.item()}\")\n",
    "print(f\"∂f/∂x: {x.grad.item()}\")\n",
    "print(f\"∂f/∂y: {'Not computed (requires_grad=False)'}\")\n",
    "\n",
    "# We can also temporarily disable gradient tracking with torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    # No operations here will track gradients\n",
    "    z = x * y\n",
    "    print(f\"\\nComputed z = x * y = {z.item()} without tracking gradients\")\n",
    "    print(f\"z.requires_grad: {z.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7ffc1c",
   "metadata": {},
   "source": [
    "### 4.3 Exercise: Computing and Comparing Gradients\n",
    "\n",
    "In this exercise, you'll explore how gradients change for different input values:\n",
    "\n",
    "1. Pick a function of your choice (e.g., $f(x, y) = \\sin(x + y) * e^{xy}$)\n",
    "2. Compute its gradients at three different points\n",
    "3. Visualize how the gradient field changes across the input space\n",
    "4. Identify regions where the gradient is large or small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94686a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90a1c4f",
   "metadata": {},
   "source": [
    "### Solution 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f496119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Choose a function\n",
    "# f(x, y) = sin(x + y) * e^(xy)\n",
    "\n",
    "# Define the function\n",
    "def f_exercise(x, y):\n",
    "    return torch.sin(x + y) * torch.exp(x * y)\n",
    "\n",
    "# 2. Compute gradients at three different points\n",
    "def compute_gradient_at_point(f, x_val, y_val):\n",
    "    x = torch.tensor(x_val, requires_grad=True)\n",
    "    y = torch.tensor(y_val, requires_grad=True)\n",
    "    \n",
    "    z = f(x, y)\n",
    "    z.backward()\n",
    "    \n",
    "    return {\n",
    "        'point': (x_val, y_val),\n",
    "        'f_value': z.item(),\n",
    "        'grad_x': x.grad.item(),\n",
    "        'grad_y': y.grad.item()\n",
    "    }\n",
    "\n",
    "# Compute at three points\n",
    "point1 = compute_gradient_at_point(f_exercise, 0.0, 0.0)\n",
    "point2 = compute_gradient_at_point(f_exercise, 1.0, 1.0)\n",
    "point3 = compute_gradient_at_point(f_exercise, -1.0, 1.0)\n",
    "\n",
    "# Print results\n",
    "for i, point in enumerate([point1, point2, point3], 1):\n",
    "    print(f\"Point {i}: ({point['point'][0]}, {point['point'][1]})\")\n",
    "    print(f\"  Function value: {point['f_value']:.4f}\")\n",
    "    print(f\"  ∂f/∂x: {point['grad_x']:.4f}\")\n",
    "    print(f\"  ∂f/∂y: {point['grad_y']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# 3. Visualize gradients across input space\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Create a grid of points\n",
    "x_vals = np.linspace(-2, 2, 30)\n",
    "y_vals = np.linspace(-2, 2, 30)\n",
    "X, Y = np.meshgrid(x_vals, y_vals)\n",
    "\n",
    "# Compute function values and gradients\n",
    "Z = np.zeros_like(X)\n",
    "grad_x = np.zeros_like(X)\n",
    "grad_y = np.zeros_like(X)\n",
    "grad_magnitude = np.zeros_like(X)\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        x_tensor = torch.tensor(X[i, j], requires_grad=True)\n",
    "        y_tensor = torch.tensor(Y[i, j], requires_grad=True)\n",
    "        \n",
    "        # Compute function value\n",
    "        z = f_exercise(x_tensor, y_tensor)\n",
    "        Z[i, j] = z.item()\n",
    "        \n",
    "        # Compute gradients\n",
    "        z.backward()\n",
    "        grad_x[i, j] = x_tensor.grad.item()\n",
    "        grad_y[i, j] = y_tensor.grad.item()\n",
    "        grad_magnitude[i, j] = np.sqrt(x_tensor.grad.item()**2 + y_tensor.grad.item()**2)\n",
    "        \n",
    "        # Reset gradients for next iteration\n",
    "        x_tensor.grad.zero_()\n",
    "        y_tensor.grad.zero_()\n",
    "\n",
    "# Contour plot with gradient field\n",
    "contour = ax1.contourf(X, Y, Z, 20, cmap=cm.coolwarm)\n",
    "ax1.quiver(X, Y, grad_x, grad_y, color='black', scale=50)\n",
    "\n",
    "# Mark the three points\n",
    "for point, color, marker in zip([point1, point2, point3], ['red', 'green', 'blue'], ['o', 's', '^']):\n",
    "    x_val, y_val = point['point']\n",
    "    ax1.plot(x_val, y_val, color=color, marker=marker, markersize=10)\n",
    "    ax1.quiver(x_val, y_val, point['grad_x'], point['grad_y'], color=color, scale=20)\n",
    "    ax1.text(x_val+0.1, y_val+0.1, f'∇f{point[\"point\"]}', color=color, fontsize=10)\n",
    "\n",
    "fig.colorbar(contour, ax=ax1)\n",
    "ax1.set_xlabel('$x$')\n",
    "ax1.set_ylabel('$y$')\n",
    "ax1.set_title('Function $f(x,y) = \\sin(x+y) \\cdot e^{xy}$ with Gradient Field')\n",
    "\n",
    "# 4. Plot gradient magnitude to identify regions of large/small gradients\n",
    "contour2 = ax2.contourf(X, Y, grad_magnitude, 20, cmap='viridis')\n",
    "fig.colorbar(contour2, ax=ax2)\n",
    "ax2.set_xlabel('$x$')\n",
    "ax2.set_ylabel('$y$')\n",
    "ax2.set_title('Gradient Magnitude $|\\\\nabla f(x,y)|$')\n",
    "\n",
    "# Mark regions of interest\n",
    "# Find location of maximum gradient in our grid\n",
    "i_max, j_max = np.unravel_index(grad_magnitude.argmax(), grad_magnitude.shape)\n",
    "x_max, y_max = X[i_max, j_max], Y[i_max, j_max]\n",
    "ax2.plot(x_max, y_max, 'r*', markersize=15)\n",
    "ax2.text(x_max+0.1, y_max+0.1, 'Max gradient', color='red', fontsize=12)\n",
    "\n",
    "# Find a location with small gradient (near origin)\n",
    "ax2.plot(0, 0, 'bo', markersize=10)\n",
    "ax2.text(0.1, 0.1, 'Small gradient', color='blue', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e9b0e5",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lecture, we've explored:\n",
    "\n",
    "1. **Computation Graphs**: How PyTorch represents mathematical operations as a network\n",
    "\n",
    "2. **Automatic Differentiation**: How PyTorch computes gradients by traversing the computation graph and applying the chain rule\n",
    "\n",
    "3. **Benefits of Autograd for Complex Functions**: How autograd helps with complicated derivatives\n",
    "\n",
    "4. **Working with Autodiff**: Practical considerations for gradient computation\n",
    "\n",
    "These concepts form the foundation of deep learning optimization. When we train neural networks, we're essentially:\n",
    "\n",
    "1. Building a computation graph (the forward pass)\n",
    "2. Computing gradients (the backward pass)\n",
    "3. Updating parameters based on these gradients (optimization)\n",
    "\n",
    "Understanding these concepts will help you debug and improve your neural network implementations!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
