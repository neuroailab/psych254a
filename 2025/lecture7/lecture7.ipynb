{"cells":[{"cell_type":"code","execution_count":1,"id":"c19ace6b","metadata":{"lines_to_next_cell":1,"colab":{"base_uri":"https://localhost:8080/"},"id":"c19ace6b","executionInfo":{"status":"ok","timestamp":1745529518139,"user_tz":420,"elapsed":42837,"user":{"displayName":"Daniel Wurgaft","userId":"06224412301086069849"}},"outputId":"050a1d7c-546d-4cbc-cdbf-c9de65f79911"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]},{"output_type":"execute_result","data":{"text/plain":["RandomState(MT19937) at 0x7F631FB4AC40"]},"metadata":{},"execution_count":1}],"source":["# Set up the environment with necessary imports\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import os\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import r2_score\n","from sklearn.preprocessing import StandardScaler\n","from tqdm.notebook import tqdm\n","from sklearn.utils import check_random_state\n","from sklearn.model_selection import ShuffleSplit\n","import copy\n","\n","# Set plot style\n","sns.set_style(\"whitegrid\")\n","\n","# Mount drive (if using Google Colab)\n","from google.colab import drive\n","drive.mount(\"/content/drive/\")\n","DATA_DIRECTORY = \"/content/drive/MyDrive/psych254a_2025/data\"\n","\n","# set random seed\n","SEED = 111\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","check_random_state(SEED)"]},{"cell_type":"markdown","id":"32e50dd7","metadata":{"id":"32e50dd7"},"source":["# Regression Models in Behavioral Science\n","\n","In this lecture, we will explore different regression techniques using PyTorch:\n","\n","1. **Training and Evaluation Fundamentals**: Creating reusable functions\n","2. **Linear and Logistic Growth Models**: Comparing different functional forms\n","3. **Multivariate Regression**: Working with multiple predictors\n","4. **Multiple Response Variables**: Predicting multiple outcomes simultaneously\n","\n","We'll build on your existing knowledge of PyTorch models and optimization, focusing on applying these techniques to behavioral data."]},{"cell_type":"markdown","id":"cb207ac3","metadata":{"id":"cb207ac3"},"source":["## 1. Training and Evaluation Fundamentals\n","\n","Before we dive into specific models, let's create reusable functions for training and evaluating models. This will help streamline our workflow for all subsequent exercises.\n","\n","### Exercise 1.1: Implement a Training Method\n","\n","Create a `train_model` method within the UniversalProcedure class that trains a PyTorch model using gradient descent. Make sure to use the loss_func and optimizer defined within the class init!"]},{"cell_type":"code","source":["class UniversalProcedure:\n","    \"\"\"A class to implement the universal procedure for model training and evaluation.\"\"\"\n","\n","    def __init__(self, cross_validator,\n","                 evaluation_metrics=None, loss_func=None, optimizer=None):\n","        self.cross_validator = cross_validator\n","\n","        if evaluation_metrics is None:\n","            self.evaluation_metrics = {\n","                'MSE': nn.MSELoss(),\n","                'R^2': lambda y, y_pred: 1 - torch.sum((y - y_pred)**2) / torch.sum((y - torch.mean(y))**2)\n","            }\n","        else:\n","            self.evaluation_metrics = evaluation_metrics\n","\n","        if loss_func is None:\n","            self.loss_func = nn.MSELoss()\n","        else:\n","            self.loss_func = loss_func\n","\n","        if optimizer is None:\n","            self.optimizer = optim.Adam\n","        else:\n","            self.optimizer = optimizer\n","\n","\n","    def train(self, model, X_train, y_train, train_epochs, lr):\n","\n","        # --- YOUR CODE HERE --- If you need a refresher: go to part 3 of the lecture 5 notebook!\n","        pass # delete this\n","        # set up optimizer and loss function from self\n","\n","        # Track losses during training by appending to a list\n","\n","        # Training loop\n","\n","            # Forward pass\n","\n","            # Compute loss\n","\n","\n","            # Backward pass and optimize\n","\n","        # ----------------------\n","\n","    def evaluate(self, model, x, y, train_epochs=500, lr=0.01):\n","\n","        # Initialize results dictionary\n","        results = {}\n","        for name in self.evaluation_metrics.keys():\n","            results[f'splits_{name}'] = []\n","\n","        # get default params from model\n","        original_state_dict = copy.deepcopy(model.state_dict())  # This is a reference so we can reset params later\n","\n","        # state_dict lst\n","        state_dicts = []\n","\n","        # Perform cross-validation\n","        for train_idx, test_idx in tqdm(self.cross_validator.split(x)):\n","\n","            # --- YOUR CODE HERE ---\n","            # Split data\n","            x_train, x_test = x[train_idx], x[test_idx]\n","            y_train, y_test = y[train_idx], y[test_idx]\n","\n","            # reset model params\n","            model.load_state_dict(original_state_dict)\n","\n","            # Fit model\n","            self.train(model, x_train, y_train, train_epochs, lr)\n","\n","            # Get predictions\n","            with torch.no_grad():\n","              y_test_pred = model(x_test)\n","            # ----------------------\n","\n","            # Calculate metrics\n","            for name, metric_fn in self.evaluation_metrics.items():\n","                results[f'splits_{name}'].append(metric_fn(y_test, y_test_pred))\n","\n","            state_dicts.append(copy.deepcopy(model.state_dict()))\n","\n","        # Average metrics across folds\n","        for name in self.evaluation_metrics.keys():\n","\n","            results[f'CV {name}'] = np.mean(results[f'splits_{name}'])\n","            results[f'CV {name} Std'] = np.std(results[f'splits_{name}'])\n","\n","        return results, state_dicts"],"metadata":{"id":"gH6_-Alc0PGv","executionInfo":{"status":"ok","timestamp":1745529518140,"user_tz":420,"elapsed":3,"user":{"displayName":"Daniel Wurgaft","userId":"06224412301086069849"}}},"id":"gH6_-Alc0PGv","execution_count":2,"outputs":[]},{"cell_type":"markdown","id":"ef1e5712","metadata":{"id":"ef1e5712"},"source":["## 2. Linear and Logistic Growth Models\n","\n","Now, let's apply our training and evaluation functions to fit linear and logistic growth models to language acquisition data from Wordbank.\n","\n","First, let's load the Wordbank data:"]},{"cell_type":"code","execution_count":null,"id":"77364ea8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"77364ea8","executionInfo":{"status":"ok","timestamp":1745519665761,"user_tz":420,"elapsed":422,"user":{"displayName":"Daniel Wurgaft","userId":"06224412301086069849"}},"outputId":"dcd2ca90-fcf1-4802-e21d-c8ed5ba36227"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wordbank data shape: (96246, 22)\n","\n","First few rows:\n","   downloaded        language     form dataset_name  child_id  age  \\\n","0  2025-04-01  Arabic (Saudi)  WSOther         JISH     87537    8   \n","1  2025-04-01  Arabic (Saudi)  WSOther         JISH     87538    8   \n","2  2025-04-01  Arabic (Saudi)  WSOther         JISH     87539   10   \n","3  2025-04-01  Arabic (Saudi)  WSOther         JISH     87540   11   \n","4  2025-04-01  Arabic (Saudi)  WSOther         JISH     87541   11   \n","\n","   comprehension  production  is_norming birth_order  ... race   sex  \\\n","0            NaN         NaN       False       First  ...  NaN  Male   \n","1            NaN         NaN       False      Second  ...  NaN  Male   \n","2            NaN         NaN       False       First  ...  NaN  Male   \n","3            NaN         NaN       False       Third  ...  NaN  Male   \n","4            NaN         NaN       False       Fifth  ...  NaN  Male   \n","\n","  birth_weight born_early_or_late  gestational_age  zygosity  \\\n","0          NaN                NaN              NaN       NaN   \n","1          NaN                NaN              NaN       NaN   \n","2          NaN                NaN              NaN       NaN   \n","3          NaN                NaN              NaN       NaN   \n","4          NaN                NaN              NaN       NaN   \n","\n","   language_exposures  health_conditions monolingual  typically_developing  \n","0                 NaN                NaN        True                  True  \n","1                 NaN                NaN        True                  True  \n","2                 NaN                NaN        True                  True  \n","3                 NaN                NaN        True                  True  \n","4                 NaN                NaN        True                  True  \n","\n","[5 rows x 22 columns]\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-11-0708fa3b9d53>:2: DtypeWarning: Columns (11,18) have mixed types. Specify dtype option on import or set low_memory=False.\n","  wordbank_data = pd.read_csv(os.path.join(DATA_DIRECTORY, \"wordbank_bychild.csv\"))\n"]}],"source":["# Load the Wordbank data\n","wordbank_data = pd.read_csv(os.path.join(DATA_DIRECTORY, \"wordbank_bychild.csv\"))\n","print(\"Wordbank data shape:\", wordbank_data.shape)\n","print(\"\\nFirst few rows:\")\n","print(wordbank_data.head())"]},{"cell_type":"code","execution_count":null,"id":"7a1fb567","metadata":{"lines_to_next_cell":1,"id":"7a1fb567"},"outputs":[],"source":["# Filter for rows with non-null production values\n","wordbank_filtered = wordbank_data.dropna(subset=[\"production\"])\n","\n","# Calculate summary statistics by language and age\n","production_averages = {}\n","\n","# Get summary stats for each language\n","for language in wordbank_filtered[\"language\"].unique():\n","    # Get data just for this language\n","    summary_data_by_child = wordbank_filtered[wordbank_filtered[\"language\"] == language]\n","\n","    # Skip languages with too little data\n","    if len(summary_data_by_child) < 10:\n","        continue\n","\n","    # Produce summaries of the mean, std, and sample length, grouped by age bin\n","    prod_means = summary_data_by_child[[\"age\", \"production\"]].groupby([\"age\"], as_index=False).mean()\n","    prod_stds = summary_data_by_child[[\"age\", \"production\"]].groupby([\"age\"], as_index=False).std()\n","    prod_lens = summary_data_by_child[[\"age\", \"production\"]].groupby([\"age\"], as_index=False).agg(len)\n","\n","    # Get the independent variable\n","    ages = prod_means[\"age\"]\n","\n","    # Get the dependent variable mean\n","    means = prod_means[\"production\"]\n","\n","    # Get the dependent variable SEMs\n","    stds = prod_stds[\"production\"]\n","    lens = prod_lens[\"production\"]\n","    sems = stds / np.sqrt(lens)\n","\n","    # Store computed things for future use\n","    production_averages[language] = (ages, means, sems)"]},{"cell_type":"markdown","id":"085b9937","metadata":{"id":"085b9937"},"source":["### Exercise 2.1: Linear Growth Model\n","\n","Implement a linear growth model as a PyTorch class, following the form $f(x) = ax + b$."]},{"cell_type":"code","execution_count":null,"id":"ff662dc2","metadata":{"lines_to_next_cell":1,"id":"ff662dc2"},"outputs":[],"source":["class LinearGrowthModel(nn.Module):\n","  ## YOUR CODE HERE ##\n","  pass # delete this\n","  # remember what the components of a pytorch class should be? if not, go to lecture 2 notebook!"]},{"cell_type":"markdown","id":"8af5f6a9","metadata":{"id":"8af5f6a9"},"source":["### Exercise 2.2: Logistic Growth Model\n","\n","Implement a logistic growth model as a PyTorch class, following the form:\n","\n","$$f(x) = \\frac{c}{1 + e^{-b(x-a)}}$$\n","\n","Where:\n","- a: Midpoint (age at which production is 50% of maximum)\n","- b: Steepness (higher value = steeper curve)\n","- c: Maximum value (asymptote)"]},{"cell_type":"code","execution_count":null,"id":"29e10dbc","metadata":{"lines_to_next_cell":1,"id":"29e10dbc"},"outputs":[],"source":["class LogisticGrowthModel(nn.Module):\n","    ## YOUR CODE HERE ##\n","    pass # delete this\n","    # remember what the components of a pytorch class should be? if not, go to lecture 2 notebook!"]},{"cell_type":"markdown","id":"49def493","metadata":{"id":"49def493"},"source":["### Exercise 2.3: Compare Linear and Logistic Models\n","\n","Train and evaluate both the linear and logistic growth models on language acquisition data for English.\n","Compare their performance using your `train_model` and `eval_model` functions."]},{"cell_type":"code","execution_count":null,"id":"2d4d51d3","metadata":{"id":"2d4d51d3"},"outputs":[],"source":["# Extract English data\n","language = \"English (American)\"\n","ages, means, sems = production_averages[language]\n","\n","# Convert to PyTorch tensors\n","ages_tensor = torch.tensor(ages.values, dtype=torch.float32).reshape(-1, 1)\n","means_tensor = torch.tensor(means.values, dtype=torch.float32).reshape(-1, 1)\n","\n","# Split data - using all data for this example since we have limited points\n","# In practice, you would use train/test split\n","X_train, y_train = ages_tensor, means_tensor\n","\n","## YOUR CODE HERE ##\n","\n","# create cross validator instance - if you don't remember how to do this, go to lecture 6 notebook and search for ShuffleSplit\n","\n","# instantiate a UniversalProcedure instance\n","\n","# create linear and logistic model instances\n","\n","# Train and Evaluate models via the universal procedure, by calling the evaluate method\n","\n","# print evaluation results - uncomment this\n","# print(f\"Linear Model: CV R^2: {linear_results['CV R^2']} (Std={linear_results['CV R^2 Std']}); CV MSE: {linear_results['CV MSE']} (Std={linear_results['CV MSE Std']});\")\n","# print(f\"Logistic Model: CV R^2: {logistic_results['CV R^2']} (Std={logistic_results['CV R^2 Std']}); CV MSE: {logistic_results['CV MSE']} (Std={logistic_results['CV MSE Std']});\")"]},{"cell_type":"markdown","id":"3a20a847","metadata":{"id":"3a20a847"},"source":["### Exercise 2.4: Compare Models Across Languages\n","\n","Choose another language and repeat the comparison between linear and logistic growth models.\n","Does the same model perform better across different languages?"]},{"cell_type":"code","execution_count":null,"id":"eb4f02f3","metadata":{"id":"eb4f02f3"},"outputs":[],"source":["# Choose another language and compare models\n","## YOUR CODE HERE ##\n","# you can simply copy the code above and change the language parameter"]},{"cell_type":"markdown","id":"e454e3ff","metadata":{"id":"e454e3ff"},"source":["## 3. Multivariate Regression\n","\n","We'll now move to multivariate regression, where we have multiple predictor variables.\n","\n","In multivariate regression, our model takes the form:\n","\n","$$\\hat{y} = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b$$\n","\n","This can be expressed in matrix form as:\n","\n","$$\\hat{y} = X \\mathbf{w} + b$$\n","\n","Where:\n","- $X$ is the design matrix with shape (n_samples, n_features)\n","- $\\mathbf{w}$ is the weight vector with shape (n_features, 1)\n","- $b$ is the bias term\n","\n","Let's implement this using the SRO dataset."]},{"cell_type":"code","execution_count":null,"id":"8ade988e","metadata":{"lines_to_next_cell":1,"colab":{"base_uri":"https://localhost:8080/"},"id":"8ade988e","executionInfo":{"status":"ok","timestamp":1745519644984,"user_tz":420,"elapsed":902,"user":{"displayName":"Daniel Wurgaft","userId":"06224412301086069849"}},"outputId":"f07ebf41-d445-4a15-84e9-c8106201a5fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["##############Health columns:#################\n","\n","['health_Nervous', 'health_Hopeless', 'health_RestlessFidgety', 'health_Depressed', 'health_EverythingIsEffort', 'health_Worthless', 'health_Last30DaysUsual', 'health_DaysLostLastMonth', 'health_DaysHalfLastMonth', 'health_DoctorVisitsLastMonth', 'health_DaysPhysicalHealthFeelings', 'health_PsychDiagnoses', 'health_PsychDiagnosesOther', 'health_NeurologicalDiagnoses', 'health_NeurologicalDiagnosesDescribe', 'health_DiseaseDiagnoses', 'health_DiseaseDiagnosesOther', 'dietary_decision.health_sensitivity', 'dospert_eb_survey.health_safety.logTr', 'dospert_rp_survey.health_safety', 'dospert_rt_survey.health_safety']\n","\n","##############Survey columns:#################\n","\n","['bis11_survey.Attentional', 'bis11_survey.Motor.logTr', 'bis11_survey.Nonplanning', 'bis_bas_survey.BAS_drive', 'bis_bas_survey.BAS_fun_seeking', 'bis_bas_survey.BAS_reward_responsiveness', 'bis_bas_survey.BIS', 'brief_self_control_survey.self_control', 'cognitive_reflection_survey.correct_proportion', 'cognitive_reflection_survey.intuitive_proportion', 'dickman_survey.functional', 'dospert_eb_survey.ethical', 'dospert_eb_survey.financial', 'dospert_eb_survey.health_safety.logTr', 'dospert_eb_survey.recreational', 'dospert_eb_survey.social', 'dospert_rp_survey.ethical', 'dospert_rp_survey.financial', 'dospert_rp_survey.health_safety', 'dospert_rp_survey.recreational', 'dospert_rp_survey.social', 'dospert_rt_survey.ethical', 'dospert_rt_survey.financial', 'dospert_rt_survey.health_safety', 'dospert_rt_survey.recreational', 'dospert_rt_survey.social', 'eating_survey.cognitive_restraint', 'eating_survey.emotional_eating', 'eating_survey.uncontrolled_eating', 'erq_survey.reappraisal', 'erq_survey.suppression', 'five_facet_mindfulness_survey.act_with_awareness', 'five_facet_mindfulness_survey.describe', 'five_facet_mindfulness_survey.nonjudge', 'five_facet_mindfulness_survey.nonreact', 'five_facet_mindfulness_survey.observe', 'future_time_perspective_survey.future_time_perspective', 'grit_scale_survey.grit', 'holt_laury_survey.beta.logTr', 'holt_laury_survey.risk_aversion', 'holt_laury_survey.safe_choices', 'impulsive_venture_survey.venturesomeness', 'mindful_attention_awareness_survey.mindfulness', 'mpq_control_survey.control.ReflogTr', 'selection_optimization_compensation_survey.compensation', 'selection_optimization_compensation_survey.elective_selection', 'selection_optimization_compensation_survey.loss_based_selection', 'selection_optimization_compensation_survey.optimization.ReflogTr', 'self_regulation_survey.control', 'sensation_seeking_survey.boredom_susceptibility', 'sensation_seeking_survey.disinhibition', 'sensation_seeking_survey.experience_seeking', 'sensation_seeking_survey.thrill_adventure_seeking', 'ten_item_personality_survey.agreeableness', 'ten_item_personality_survey.conscientiousness.ReflogTr', 'ten_item_personality_survey.emotional_stability', 'ten_item_personality_survey.extraversion', 'ten_item_personality_survey.openness', 'theories_of_willpower_survey.endorse_limited_resource', 'time_perspective_survey.future', 'time_perspective_survey.past_negative', 'time_perspective_survey.past_positive', 'time_perspective_survey.present_fatalistic', 'time_perspective_survey.present_hedonistic', 'upps_impulsivity_survey.lack_of_perseverance', 'upps_impulsivity_survey.lack_of_premeditation', 'upps_impulsivity_survey.negative_urgency', 'upps_impulsivity_survey.positive_urgency', 'upps_impulsivity_survey.sensation_seeking']\n","\n","##############Task columns:#################\n","\n","['angling_risk_task_always_sunny.keep_adjusted_clicks', 'angling_risk_task_always_sunny.keep_coef_of_variation', 'angling_risk_task_always_sunny.release_adjusted_clicks', 'angling_risk_task_always_sunny.release_coef_of_variation.logTr', 'attention_network_task.alerting_hddm_drift', 'attention_network_task.conflict_hddm_drift.ReflogTr', 'attention_network_task.hddm_drift', 'attention_network_task.hddm_non_decision.ReflogTr', 'attention_network_task.hddm_thresh', 'attention_network_task.orienting_hddm_drift', 'columbia_card_task_cold.avg_cards_chosen', 'columbia_card_task_cold.gain_sensitivity.logTr', 'columbia_card_task_cold.information_use', 'columbia_card_task_cold.loss_sensitivity', 'columbia_card_task_cold.probability_sensitivity', 'columbia_card_task_hot.avg_cards_chosen', 'columbia_card_task_hot.gain_sensitivity.logTr', 'columbia_card_task_hot.information_use', 'columbia_card_task_hot.loss_sensitivity.ReflogTr', 'columbia_card_task_hot.probability_sensitivity', 'information_sampling_task.Decreasing_Win_P_correct', 'information_sampling_task.Decreasing_Win_motivation', 'information_sampling_task.Fixed_Win_P_correct', 'information_sampling_task.Fixed_Win_motivation', 'shift_task.acc', 'shift_task.learning_rate', 'shift_task.learning_to_learn', 'shift_task.model_beta.logTr', 'shift_task.model_decay.ReflogTr', 'shift_task.model_learning_rate', 'threebytwo.task_switch_cost_hddm_drift', 'writing_task.neutral_probability', 'writing_task.positive_probability']\n"]}],"source":["# Load the SRO data\n","sro_datadir = os.path.join(DATA_DIRECTORY, \"SRO\")\n","health = pd.read_csv(os.path.join(sro_datadir, \"health.csv\"), index_col=0)\n","# add health before each health variable\n","health.columns = [\"health_\" + col for col in health.columns]\n","meaningful_vars = pd.read_csv(os.path.join(sro_datadir, \"meaningful_variables_clean.csv\"), index_col=0)\n","\n","# Join the data\n","joined = health.join(meaningful_vars)\n","\n","# show all columns\n","import sys\n","np.set_printoptions(threshold=sys.maxsize)\n","print(\"##############Health columns:#################\\n\")\n","all_health_features = [col for col in joined.columns if \"health\" in col]\n","print(all_health_features)\n","\n","print(\"\\n##############Survey columns:#################\\n\")\n","all_survey_features = [col for col in joined.columns if \"survey\" in col]\n","print(all_survey_features)\n","\n","print(\"\\n##############Task columns:#################\\n\")\n","all_task_features = [col for col in joined.columns if \"task\" in col]\n","print(all_task_features)"]},{"cell_type":"markdown","id":"19a45725","metadata":{"id":"19a45725"},"source":["### Exercise 3.1: Implement Multivariate Linear Regression\n","\n","Implement a multivariate linear regression model that takes multiple features as input.\n","Use the design matrix approach, where the weights are represented as a vector.\n","\n","Look at the equation above for how to do so!"]},{"cell_type":"code","execution_count":null,"id":"e997c3e7","metadata":{"lines_to_next_cell":1,"id":"e997c3e7"},"outputs":[],"source":["class MultivariateLinearModel(nn.Module):\n","    def __init__(self, input_dim): # the __init__ method has to take in input_dim to know the size of the weight vector\n","      ## YOUR CODE HERE\n","      pass # delete this"]},{"cell_type":"markdown","id":"f58bf084","metadata":{"id":"f58bf084"},"source":["### Exercise 3.2: Train and Evaluate Multivariate Model\n","\n","Use your training and evaluation functions to fit the multivariate model on SRO data.\n","Predict \"health_EverythingIsEffort\" from multiple health-related features."]},{"cell_type":"code","execution_count":null,"id":"61ec472d","metadata":{"id":"61ec472d"},"outputs":[],"source":["# Prepare the data for multivariate regression\n","# Select features and target\n","features = [\"mindful_attention_awareness_survey.mindfulness\",\n","            \"ten_item_personality_survey.emotional_stability\",\n","            \"columbia_card_task_cold.loss_sensitivity\",\n","            \"probabilistic_selection.positive_learning_bias\"]\n","target = \"health_EverythingIsEffort\"\n","\n","# Drop rows with NaN values in any of these columns\n","clean_data = joined[features + [target]].dropna()\n","\n","# Create the design matrix X and target vector y\n","X = clean_data[features].values\n","y = clean_data[target].values.reshape(-1, 1)\n","\n","# Scale the features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Convert to PyTorch tensors\n","X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n","y_tensor = torch.tensor(y, dtype=torch.float32)\n","\n","## YOUR CODE HERE ##\n","\n","# create cross validator instance\n","\n","# instantiate the UniversalProcedure\n","\n","# instantiate MultivariateLinearModel model instance\n","\n","# Train and Evaluate models via the universal procedure\n","\n","# # print evaluation results\n","# print(f\"Linear Model: CV R^2: {multivariate_results['CV R^2']} (Std={multivariate_results['CV R^2 Std']})\")\n","\n","# # print avg feature values\n","# avg_W = torch.mean(torch.stack([state_dict[\"W\"] for state_dict in state_dicts]), dim=0)\n","# print(\"\\nAverage Weight per feature:\\n\")\n","# print(\"\\n\".join([\": \".join([feature, str(weight)]) for feature, weight in zip(features, avg_W.squeeze().tolist())]))"]},{"cell_type":"markdown","id":"9af45b2b","metadata":{"id":"9af45b2b"},"source":["## 4. Multiple Response Variables\n","\n","Finally, let's extend our model to handle multiple response variables. In this case, both our input X and output Y are matrices.\n","\n","The model takes the form:\n","\n","$$\\hat{Y} = X W + B$$\n","\n","Where:\n","- $X$ is the design matrix with shape (n_samples, n_features)\n","- $W$ is the weight matrix with shape (n_features, n_targets)\n","- $B$ is the bias matrix (or vector broadcast across samples)\n","\n","This generalizes the multivariate case to predict multiple outcomes simultaneously."]},{"cell_type":"markdown","id":"72edef6f","metadata":{"id":"72edef6f"},"source":["### Exercise 4.1: Implement Multiple Response Model\n","\n","Implement a model that can predict multiple response variables simultaneously using standard matrix multiplication."]},{"cell_type":"code","execution_count":null,"id":"9cda5a76","metadata":{"lines_to_next_cell":1,"id":"9cda5a76"},"outputs":[],"source":["class MultipleResponseModel(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        \"\"\"\n","        Initialize a model for multiple response variables.\n","\n","        Args:\n","            input_dim: Number of input features\n","            output_dim: Number of output targets\n","        \"\"\"\n","        ## YOUR CODE HERE ##\n","        pass # delete this"]},{"cell_type":"markdown","id":"35456602","metadata":{"id":"35456602"},"source":["### Exercise 4.2: Train and Evaluate Multiple Response Model\n","\n","Use your training and evaluation functions to predict multiple health outcomes simultaneously."]},{"cell_type":"code","execution_count":null,"id":"339febe3","metadata":{"id":"339febe3","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1745476324738,"user_tz":420,"elapsed":97,"user":{"displayName":"Daniel Wurgaft","userId":"06224412301086069849"}},"outputId":"0856e7af-9e2d-4294-958e-f50473408b09"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'joined' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-c6edbb0f3c08>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Drop rows with NaN values in any of these columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mclean_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Create the design matrix X and target matrix Y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'joined' is not defined"]}],"source":["# Prepare the data for multiple response regression\n","# Select features and targets\n","features = [\"mindful_attention_awareness_survey.mindfulness\",\n","            \"ten_item_personality_survey.emotional_stability\",\n","            \"columbia_card_task_cold.loss_sensitivity\",\n","            \"probabilistic_selection.positive_learning_bias\"]\n","targets = [\"health_EverythingIsEffort\", \"health_Depressed\", \"health_Nervous\"]\n","\n","# Drop rows with NaN values in any of these columns\n","clean_data = joined[features + targets].dropna()\n","\n","# Create the design matrix X and target matrix Y\n","X = clean_data[features].values\n","Y = clean_data[targets].values\n","\n","# Scale the features and targets\n","X_scaler = StandardScaler()\n","Y_scaler = StandardScaler()\n","X_scaled = X_scaler.fit_transform(X)\n","Y_scaled = Y_scaler.fit_transform(Y)\n","\n","# Convert to PyTorch tensors\n","X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n","Y_tensor = torch.tensor(Y_scaled, dtype=torch.float32)\n","\n","## YOUR CODE HERE ##\n","\n","# print evaluation results - uncomment below\n","# print(f\"Multiple response model: CV R^2: {multiple_response_results['CV R^2']} (Std={multiple_response_results['CV R^2 Std']})\")"]},{"cell_type":"markdown","source":["### Exercise 4.4: Run a model with all survey data versus all task data and get to the SRO paper's conclusions!\n","\n","This exercise intentionally has less scaffolding to let you get a sense of the full process of fitting this model!"],"metadata":{"id":"ed-ORcW5uQQv"},"id":"ed-ORcW5uQQv"},{"cell_type":"code","source":["targets = [\"health_EverythingIsEffort\", \"health_Depressed\", \"health_Nervous\", \"health_Worthless\", ]"],"metadata":{"id":"GmAEfffRuhNY"},"id":"GmAEfffRuhNY","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# all survey data model\n","# Prepare the data for multiple response regression\n","# Select features and targets\n","features = all_survey_features\n","\n","# Drop rows with NaN values in any of these columns\n","clean_data = joined[features + targets].dropna()\n","\n","# Create the design matrix X and target matrix Y\n","X = clean_data[features].values\n","Y = clean_data[targets].values\n","\n","# Scale the features and targets\n","X_scaler = StandardScaler()\n","Y_scaler = StandardScaler()\n","X_scaled = X_scaler.fit_transform(X)\n","Y_scaled = Y_scaler.fit_transform(Y)\n","\n","# Convert to PyTorch tensors\n","X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n","Y_tensor = torch.tensor(Y_scaled, dtype=torch.float32)\n","\n","## YOUR CODE HERE ##\n","\n","# create cross validator instance\n","\n","# instantiate the UniversalProcedure\n","\n","# instantiate MultivariateLinearModel model instance\n","\n","# Train and Evaluate models via the universal procedure\n","\n","# print evaluation results - uncomment below\n","# print(f\"Multiple response model: CV R^2: {multiple_response_results['CV R^2']} (Std={multiple_response_results['CV R^2 Std']})\")"],"metadata":{"id":"YI2Xn-2XvIgt"},"id":"YI2Xn-2XvIgt","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# all task data model\n","# Prepare the data for multiple response regression\n","# Select features and targets\n","features = all_task_features\n","targets = [\"health_EverythingIsEffort\", \"health_Depressed\", \"health_Nervous\"]\n","\n","# Drop rows with NaN values in any of these columns\n","clean_data = joined[features + targets].dropna()\n","\n","# Create the design matrix X and target matrix Y\n","X = clean_data[features].values\n","Y = clean_data[targets].values\n","\n","# Scale the features and targets\n","X_scaler = StandardScaler()\n","Y_scaler = StandardScaler()\n","X_scaled = X_scaler.fit_transform(X)\n","Y_scaled = Y_scaler.fit_transform(Y)\n","\n","# Convert to PyTorch tensors\n","X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n","Y_tensor = torch.tensor(Y_scaled, dtype=torch.float32)\n","\n","## YOUR CODE HERE ##\n","\n","# create cross validator instance\n","\n","# instantiate the UniversalProcedure\n","\n","# instantiate MultivariateLinearModel model instance\n","\n","# Train and Evaluate models via the universal procedure\n","\n","# print evaluation results - uncomment below\n","# print(f\"Multiple response model: CV R^2: {multiple_response_results['CV R^2']} (Std={multiple_response_results['CV R^2 Std']})\")"],"metadata":{"id":"XYvQ38BJvLpo"},"id":"XYvQ38BJvLpo","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exercise 4.4: add regularization to improve your predictions\n","\n","We will get to why this works in future classes...\n","\n","Instructions:\n","\n","1. Copy your Universal Procedure class here.\n","\n","2. In the __init__ method, add a new argument called l1_regularization, if it set to true, set self.l1_regularization = True\n","\n","3. In the train_model method, after computing the loss and before the gradient step, add an if clause for if self.l1_regularization\n","\n","4. Inside the if clause, define a variable called l1_term, and set it equal to the sum of the absolute values of all model parameters (HINT: use model parameters(), torch.sum and torch.abs)\n","\n","5. Again inside the if clause, set loss = loss + 0.01*l1_term\n","\n","6. Rerun the chunk containing the updated universal procedure\n","\n","7. Copy the two previous cells containing the models with all survey variables and all task variables to the code cells below, set l1_regularization = True when instantiating the Universal procedure class, and rerun them.\n","\n","8. Do you see any differences? what changed?"],"metadata":{"id":"t8NI--P7xRuA"},"id":"t8NI--P7xRuA"},{"cell_type":"code","source":["# copy universal procedure class here"],"metadata":{"id":"Wdv7_3wLygUJ"},"id":"Wdv7_3wLygUJ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# copy all survey data model"],"metadata":{"id":"lqcHQzYsGx2w"},"id":"lqcHQzYsGx2w","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# copy all task data model"],"metadata":{"id":"zPn3iP4UGzRf"},"id":"zPn3iP4UGzRf","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"45ea5632","metadata":{"id":"45ea5632"},"source":["## Conclusion\n","\n","In this notebook, we explored various regression techniques using PyTorch:\n","\n","1. **Training and Evaluation Functions**: We created reusable functions for model training and evaluation.\n","\n","2. **Growth Models**: We compared linear and logistic growth models for language acquisition.\n","\n","3. **Multivariate Regression**: We implemented regression with multiple predictors using a design matrix approach.\n","\n","4. **Multiple Response Variables**: We extended our models to predict multiple outcomes simultaneously.\n","\n","These techniques form the foundation of regression analysis in behavioral science and demonstrate the flexibility of PyTorch for implementing custom statistical models."]}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}