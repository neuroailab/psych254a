{"cells":[{"cell_type":"code","execution_count":null,"id":"f55d275d","metadata":{"deletable":true,"editable":true,"id":"f55d275d","lines_to_next_cell":1},"outputs":[],"source":["import matplotlib\n","import matplotlib.pyplot as plt\n","import time\n","import numpy as np\n","import scipy as sp\n","import scipy.stats as stats\n","import h5py\n","import torch\n","import copy\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm\n","from sklearn.manifold import MDS\n","from sklearn.model_selection import ShuffleSplit\n","import os\n","from google.colab import drive\n","import seaborn as sns\n","\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":null,"id":"b7d3f73e","metadata":{"deletable":true,"editable":true,"id":"b7d3f73e","lines_to_next_cell":1},"outputs":[],"source":["# Universal Procedure class from lecture 7\n","class UniversalProcedure:\n","    \"\"\"A class to implement the universal procedure for model training and evaluation.\"\"\"\n","\n","    def __init__(\n","        self, cross_validator, evaluation_metrics=None, loss_func=None, optimizer=None\n","    ):\n","\n","        self.cross_validator = cross_validator\n","\n","        if evaluation_metrics is None:\n","            self.evaluation_metrics = {\"Accuracy\": lambda y, y_pred: (y_pred == y).float().mean() }\n","        else:\n","            self.evaluation_metrics = evaluation_metrics\n","\n","        if loss_func is None:\n","            self.loss_func = nn.MSELoss()\n","        else:\n","            self.loss_func = loss_func\n","\n","        if optimizer is None:\n","            self.optimizer = optim.Adam\n","        else:\n","            self.optimizer = optimizer\n","\n","    def train(\n","        self,\n","        model,  # The instantiated but untrained PyTorch model\n","        X_train,  # The training data input\n","        y_train,  # The training labels (the desired output)\n","        train_epochs,  # How many epochs to train for\n","        lr,  # The learning rate to use\n","    ):\n","\n","        # Set up optimizer and loss function from self\n","        optimizer = self.optimizer(model.parameters(), lr=lr)\n","        loss_fn = self.loss_func\n","\n","        # Track losses during training\n","        losses = []\n","\n","        # Training loop\n","        for epoch in tqdm(range(train_epochs), leave=False):\n","            # Forward pass\n","            y_pred = model(X_train)  # Gets the prediction\n","\n","            # Compute loss\n","            loss = loss_fn(y_pred, y_train)  # Runs the loss function\n","            losses.append(loss.item())  # Appends the loss for later tracking purposes\n","\n","            # Backward pass and optimize\n","            optimizer.zero_grad()  # To make sure gradients don't accumulate\n","            loss.backward()  # This actually calls the derivation calculation\n","            optimizer.step()  # This actually applies the update\n","\n","        return losses\n","\n","    def evaluate(self, model, x, y, train_epochs=500, lr=0.01):\n","\n","        # Initialize results dictionary\n","        results = {}\n","        for name in self.evaluation_metrics.keys():\n","            results[f\"splits_{name}\"] = []\n","\n","        # Get default params from model\n","        original_state_dict = copy.deepcopy(\n","            model.state_dict()\n","        )  # Save initial parameters\n","\n","        # State_dict list to store trained model parameters\n","        state_dicts = []\n","\n","        # Perform cross-validation\n","        for train_idx, test_idx in tqdm(self.cross_validator.split(x)):\n","            # Split data\n","            x_train, x_test = x[train_idx], x[test_idx]\n","            y_train, y_test = y[train_idx], y[test_idx]\n","\n","            # Reset model parameters\n","            model.load_state_dict(original_state_dict)\n","\n","            # Fit model\n","            self.train(model, x_train, y_train, train_epochs, lr)\n","\n","            # Get predictions\n","            with torch.no_grad():\n","                y_test_pred = model.predict(x_test)\n","\n","            # Calculate metrics\n","            for name, metric_fn in self.evaluation_metrics.items():\n","                results[f\"splits_{name}\"].append(metric_fn(y_test, y_test_pred))\n","\n","            state_dicts.append(copy.deepcopy(model.state_dict()))\n","\n","        # Average metrics across folds\n","        for name in self.evaluation_metrics.keys():\n","            results[f\"CV {name}\"] = np.mean(results[f\"splits_{name}\"])\n","            results[f\"CV {name} Std\"] = np.std(results[f\"splits_{name}\"])\n","\n","        return results, state_dicts"]},{"cell_type":"code","execution_count":null,"id":"QCGEBX2QQf2d","metadata":{"id":"QCGEBX2QQf2d"},"outputs":[],"source":["# helper function\n","def ensure_bytes(input_string):\n","    if isinstance(input_string, bytes):\n","        return input_string\n","    elif isinstance(input_string, str):\n","        return input_string.encode(\"utf-8\")\n","    else:\n","        raise TypeError(f\"Expected str or bytes, got {type(input_string).__name__}\")"]},{"cell_type":"markdown","id":"3ed0b125","metadata":{"deletable":true,"editable":true,"id":"3ed0b125"},"source":["# Classification with PyTorch\n","\n","In this notebook, we'll explore classification using PyTorch, focusing on two popular linear classifiers:\n","\n","1. **Support Vector Machines (SVMs)** - which find a decision boundary that maximizes the margin between classes\n","2. **Logistic Regression** - which models the probability of class membership\n","\n","We'll apply these methods to neural data from visual cortex and explore how classifier performance changes with increasing stimulus variation.\n"]},{"cell_type":"markdown","id":"915ec3ff","metadata":{"deletable":true,"editable":true,"id":"915ec3ff"},"source":["## Loading and Exploring Neural Data\n","\n","We'll be working with a dataset of neural recordings from inferotemporal (IT) cortex, which is a higher-level visual area involved in object recognition. This dataset contains neural responses to images of various object categories, with different levels of variation (e.g., position, size, pose).\n"]},{"cell_type":"code","execution_count":null,"id":"00be9ec9","metadata":{"deletable":true,"editable":true,"id":"00be9ec9"},"outputs":[],"source":["# Load the neural data\n","LECTURE_DIRECTORY = \"/content/drive/MyDrive/psych254a_2025/data\"\n","\n","# %% deletable=true editable=true id=\"41a6c4e1\"\n","# Load the neural data\n","neural_data_file = os.path.join(LECTURE_DIRECTORY, \"ventral_neural_data.hdf5\")\n","Ventral_Dataset = h5py.File(neural_data_file, \"r\")\n","\n","\n","# Let's see what's in the dataset\n","print(\"Dataset structure:\")\n","print(list(Ventral_Dataset.keys()))"]},{"cell_type":"code","execution_count":null,"id":"18faf0e1","metadata":{"deletable":true,"editable":true,"id":"18faf0e1","lines_to_next_cell":1},"outputs":[],"source":["# Let's look at the category information\n","categories = Ventral_Dataset[\"image_meta\"][\"category\"][:]\n","unique_categories = np.unique(categories)\n","print(f\"Unique categories ({len(unique_categories)}):\")\n","print(unique_categories)\n","\n","# Also look at variation levels\n","var_level = Ventral_Dataset[\"image_meta\"][\"variation_level\"][:]\n","unique_var_levels = np.unique(var_level)\n","print(f\"\\nUnique variation levels ({len(unique_var_levels)}):\")\n","print(unique_var_levels)"]},{"cell_type":"code","execution_count":null,"id":"17148db0","metadata":{"deletable":true,"editable":true,"id":"17148db0","lines_to_next_cell":1},"outputs":[],"source":["# Get the neural data we'll be working with\n","# For this notebook, we'll focus on IT (inferotemporal cortex) data\n","IT_NEURONS = Ventral_Dataset[\"neural_meta\"][\"IT_NEURONS\"][:]\n","IT_Neural_Data = Ventral_Dataset[\"time_averaged_trial_averaged\"][:, IT_NEURONS]\n","\n","print(f\"IT neural data shape: {IT_Neural_Data.shape}\")\n","print(f\"Number of images: {IT_Neural_Data.shape[0]}\")\n","print(f\"Number of IT neurons: {IT_Neural_Data.shape[1]}\")"]},{"cell_type":"markdown","source":["### Let's take a look at single neuron data:"],"metadata":{"id":"luncpj9S2lYk"},"id":"luncpj9S2lYk"},{"cell_type":"code","source":["# Let's check out the time-binned trial-averaged data, which shows how neurons respond over time\n","time_binned_data = Ventral_Dataset['time_binned_trial_averaged']\n","print(\"Shape of time_binned_trial_averaged:\", time_binned_data.shape)\n","# This gives us responses for each image, time bin, and neuron\n","\n","# Let's look at a single neuron's response to a few different images over time\n","neuron_index = 10\n","example_image_indices = [0, 100, 200, 300]\n","\n","plt.figure(figsize=(12, 6))\n","for img_idx in example_image_indices:\n","    # Extract time course for this neuron/image\n","    neuron_response = time_binned_data[img_idx, :, neuron_index]\n","    plt.plot(neuron_response, label=f'Image {img_idx}')\n","\n","plt.xlabel('Time bin (20ms)', fontsize=12)\n","plt.xticks(np.arange(11), np.arange(11) * 20)\n","plt.ylabel('Neural response', fontsize=12)\n","plt.title(f'Neuron {neuron_index} Response Timecourse', fontsize=14)\n","plt.grid(True, alpha=0.3)\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"z5HjRc3a2nV3"},"id":"z5HjRc3a2nV3","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exploring Neural activity to different categories:"],"metadata":{"id":"Z9_iZQPM2rHB"},"id":"Z9_iZQPM2rHB"},{"cell_type":"code","source":["def compute_category_averages(neural_data, categories):\n","    \"\"\"\n","    Compute the average neural response for each category.\n","\n","    Parameters:\n","    neural_data: Array of shape (n_images, n_neurons) containing neural responses\n","    categories: Array of shape (n_images,) containing category labels for each image\n","\n","    Returns:\n","    Dictionary mapping category names to average responses\n","    \"\"\"\n","    neural_data_tensor = torch.tensor(neural_data, dtype=torch.float32)\n","    category_averages = {}\n","    for category in np.unique(categories):\n","        # Find images of this category\n","        category_mask = categories == category\n","\n","        # Get neural responses for those images\n","        category_responses = neural_data_tensor[category_mask]\n","\n","        # Compute average response (hint: use torch.mean along the appropriate dimension)\n","        average_response = torch.mean(category_responses, dim=0)\n","\n","        # Store in dictionary\n","        category_averages[category] = average_response\n","\n","    return category_averages"],"metadata":{"id":"99yY2Y2w2tZ5"},"id":"99yY2Y2w2tZ5","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now let's visualize\n","\n","# computing category averages\n","IT_category_averages = compute_category_averages(IT_Neural_Data, categories)\n","\n","# Create tensors containing all category averages for easier plotting\n","IT_category_means = torch.stack([IT_category_averages[cat] for cat in unique_categories])\n","\n","# Plot heatmaps\n","plt.figure(figsize=(15, 12))\n","\n","# IT heatmap\n","plt.subplot(2, 1, 2)\n","sns.heatmap(\n","    IT_category_means.numpy(),\n","    cmap=\"viridis\",\n","    xticklabels=range(0, len(IT_NEURONS)),\n","    yticklabels=[c.decode('utf-8') if isinstance(c, bytes) else c for c in unique_categories],\n",")\n","plt.title(\"Mean IT Neural Response by Category\", fontsize=15)\n","plt.xlabel(\"IT Neuron Index\", fontsize=12)\n","plt.ylabel(\"Stimulus Category\", fontsize=12)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"sNFoRBaA2uK9"},"id":"sNFoRBaA2uK9","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Project to 2 dimensions:"],"metadata":{"id":"Z4EiOfBZ22J6"},"id":"Z4EiOfBZ22J6"},{"cell_type":"code","execution_count":null,"id":"XIUPFHrYQagB","metadata":{"id":"XIUPFHrYQagB"},"outputs":[],"source":["def plot_in_2d_projection(vl, cat1, cat2):\n","    cat1 = ensure_bytes(cat1)\n","    cat2 = ensure_bytes(cat2)\n","    vl = ensure_bytes(vl)\n","    # Get the subset of data for the two categories\n","    subset_inds = (var_level == vl) & ((categories == cat1) | (categories == cat2))\n","    Neural_Data_subset = IT_Neural_Data[subset_inds]\n","    category_subset = categories[subset_inds]\n","\n","    # Using the unsupervised \"Multi-dimensional scaling\" method (MDS)\n","    distmat = 1 - np.corrcoef(Neural_Data_subset)\n","    x, y = MDS(dissimilarity=\"precomputed\").fit_transform(distmat).T\n","\n","    xb = x[category_subset == cat1]\n","    yb = y[category_subset == cat1]\n","    xc = x[category_subset == cat2]\n","    yc = y[category_subset == cat2]\n","    plt.scatter(xb, yb, color=\"b\")\n","    plt.scatter(xc, yc, color=\"r\")\n","    plt.axis(\"equal\")\n","    plt.xlabel(\"PC 1\")\n","    plt.ylabel(\"PC 2\")"]},{"cell_type":"code","execution_count":null,"id":"Yn2TBYUcQpd-","metadata":{"id":"Yn2TBYUcQpd-"},"outputs":[],"source":["cat1 = \"Animals\"\n","cat2 = \"Fruits\"\n","\n","fig = plt.figure(figsize=(16, 5))\n","plt.subplot(1, 3, 1)\n","plot_in_2d_projection(\"V0\", cat1, cat2)\n","plt.title(\"%s (blue) vs %s (red) var level %s\" % (cat1, cat2, \"V0\"))\n","\n","plt.subplot(1, 3, 2)\n","plot_in_2d_projection(\"V3\", cat1, cat2)\n","plt.title(\"%s (blue) vs %s (red) var level %s\" % (cat1, cat2, \"V3\"))\n","\n","plt.subplot(1, 3, 3)\n","plot_in_2d_projection(\"V6\", cat1, cat2)\n","plt.title(\"%s (blue) vs %s (red) var level %s\" % (cat1, cat2, \"V6\"))"]},{"cell_type":"code","execution_count":null,"id":"e_nOxTNxQpmX","metadata":{"id":"e_nOxTNxQpmX"},"outputs":[],"source":["cat1 = \"Cars\"\n","cat2 = \"Faces\"\n","\n","fig = plt.figure(figsize=(16, 5))\n","plt.subplot(1, 3, 1)\n","plot_in_2d_projection(\"V0\", cat1, cat2)\n","plt.title(\"%s (blue) vs %s (red) var level %s\" % (cat1, cat2, \"V0\"))\n","\n","plt.subplot(1, 3, 2)\n","plot_in_2d_projection(\"V3\", cat1, cat2)\n","plt.title(\"%s (blue) vs %s (red) var level %s\" % (cat1, cat2, \"V3\"))\n","\n","plt.subplot(1, 3, 3)\n","plot_in_2d_projection(\"V6\", cat1, cat2)\n","plt.title(\"%s (blue) vs %s (red) var level %s\" % (cat1, cat2, \"V6\"))"]},{"cell_type":"code","execution_count":null,"id":"hlOGjuUEQpsv","metadata":{"id":"hlOGjuUEQpsv"},"outputs":[],"source":["Scat1 = \"Faces\"\n","cat2 = \"Fruits\"\n","\n","fig = plt.figure(figsize=(16, 5))\n","plt.subplot(1, 3, 1)\n","plot_in_2d_projection(\"V0\", cat1, cat2)\n","plt.title(\"%s (blue) vs %s (red) var level %s\" % (cat1, cat2, \"V0\"))\n","\n","plt.subplot(1, 3, 2)\n","plot_in_2d_projection(\"V3\", cat1, cat2)\n","plt.title(\"%s (blue) vs %s (red) var level %s\" % (cat1, cat2, \"V3\"))\n","\n","plt.subplot(1, 3, 3)\n","plot_in_2d_projection(\"V6\", cat1, cat2)\n","plt.title(\"%s (blue) vs %s (red) var level %s\" % (cat1, cat2, \"V6\"))"]},{"cell_type":"markdown","id":"9a72a191","metadata":{"deletable":true,"editable":true,"id":"9a72a191"},"source":["We can see that at low variation (V0), the categories form distinct clusters in neural space, meaning the brain represents them as clearly separate object categories. As variation increases (V6), these clusters start to overlap, making category discrimination more challenging.\n","\n","Let's now see if we can build classifiers to distinguish between these categories, and how their performance changes with variation.\n"]},{"cell_type":"markdown","id":"27fc50cb","metadata":{"deletable":true,"editable":true,"id":"27fc50cb"},"source":["## Introduction to PyTorch Neural Network Modules\n","\n","Before we implement our classifiers, let's review how to build models in PyTorch. PyTorch provides an elegant way to define neural networks using the `nn.Module` class and pre-built layers like `nn.Linear`.\n","\n","A linear layer in PyTorch is defined as:\n","\n","```python\n","nn.Linear(in_features, out_features)\n","```\n","\n","This creates a module that performs: `y = xW^T + b`, where:\n","\n","- `x` is the input tensor of shape `(batch_size, in_features)`\n","- `W` is a learnable weight matrix of shape `(out_features, in_features)`\n","- `b` is a learnable bias vector of shape `(out_features)`\n","\n","For classification, we'll use linear layers to map from our input features (neural activity) to either:\n","\n","- A single score for binary classification (SVM)\n","- Multiple scores or probabilities for multi-class classification\n"]},{"cell_type":"markdown","id":"9810de53","metadata":{"deletable":true,"editable":true,"id":"9810de53"},"source":["## Exercise 1: Binary Classification with SVM\n","\n","We'll start with a binary classification problem: distinguishing between two categories of objects based on neural activity patterns.\n","\n","Support Vector Machines (SVMs) aim to find a hyperplane that maximizes the margin between classes. For binary classification, SVMs output a single score where:\n","\n","- Positive scores indicate one class\n","- Negative scores indicate the other class\n","\n","The key component of SVMs is the hinge loss function: `max(0, 1 - y * f(x))` where:\n","\n","- `y` is the label (-1 or 1)\n","- `f(x)` is the model's output score\n"]},{"cell_type":"code","execution_count":null,"id":"bd23b051","metadata":{"deletable":true,"editable":true,"id":"bd23b051","lines_to_next_cell":1,"executionInfo":{"status":"ok","timestamp":1746131582527,"user_tz":420,"elapsed":7,"user":{"displayName":"Daniel Wurgaft","userId":"06224412301086069849"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8aac43fa-7b51-4777-ac23-84804b1f4171"},"outputs":[{"output_type":"stream","name":"stdout","text":["Binary classification data: 640 samples, 168 features\n","Class balance: 320 vs 320\n"]}],"source":["# First, let's prepare our data for binary classification\n","def prepare_binary_data(category1, category2, vl):\n","    \"\"\"\n","    Prepare data for binary classification between two categories.\n","\n","    Args:\n","        category1: First category\n","        category2: Second category\n","        vl: Variation level\n","\n","    Returns:\n","        features_tensor: PyTorch tensor of neural data\n","        labels_tensor: PyTorch tensor of binary labels (1 for category1, -1 for category2)\n","    \"\"\"\n","    # Ensure inputs are bytes for HDF5 compatibility\n","    category1 = ensure_bytes(category1)\n","    category2 = ensure_bytes(category2)\n","    vl = ensure_bytes(vl)\n","\n","    # Get indices for the two categories at the specified variation level\n","    mask = (var_level == vl) & ((categories == category1) | (categories == category2))\n","\n","    # Extract features and labels\n","    features = IT_Neural_Data[mask]\n","\n","    # Create binary labels: 1 for category1, -1 for category2\n","    labels = np.ones(np.sum(mask))\n","    labels[categories[mask] == category2] = -1\n","\n","    # Convert to PyTorch tensors\n","    features_tensor = torch.tensor(features, dtype=torch.float32)\n","    labels_tensor = torch.tensor(labels, dtype=torch.float32)\n","\n","    return features_tensor, labels_tensor\n","\n","\n","# Choose two categories for binary classification\n","cat1 = unique_categories[0]  # e.g., 'Animals'\n","cat2 = unique_categories[1]  # e.g., 'Cars'\n","variation_level = \"V3\"  # Medium variation\n","\n","features_tensor, labels_tensor = prepare_binary_data(cat1, cat2, variation_level)\n","print(\n","    f\"Binary classification data: {features_tensor.shape[0]} samples, {features_tensor.shape[1]} features\"\n",")\n","print(\n","    f\"Class balance: {torch.sum(labels_tensor == 1).item()} vs {torch.sum(labels_tensor == -1).item()}\"\n",")"]},{"cell_type":"markdown","id":"3585366b","metadata":{"deletable":true,"editable":true,"id":"3585366b"},"source":["### Exercise 1.1: Implement Binary SVM Model\n","\n","Now, let's implement a binary SVM using PyTorch's `nn.Module` and `nn.Linear`. The model should output a single score for each input.\n"]},{"cell_type":"code","execution_count":null,"id":"49babef8","metadata":{"deletable":true,"editable":true,"id":"49babef8","lines_to_next_cell":1},"outputs":[],"source":["# Exercise 1.1: Implement Binary SVM with Linear Layer\n","class BinaryLinearSVM(nn.Module):\n","    \"\"\"\n","    Binary linear SVM classifier.\n","    \"\"\"\n","\n","    def __init__(self, input_dim):\n","        \"\"\"\n","        Initialize the SVM model.\n","\n","        Args:\n","            input_dim: Dimensionality of input features\n","        \"\"\"\n","        super(BinaryLinearSVM, self).__init__()\n","\n","        # YOUR CODE HERE\n","        # Initialize a linear layer (weights and bias) with nn.Linear\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass to compute SVM scores.\n","\n","        Args:\n","            x: Input features of shape (batch_size, input_dim)\n","\n","        Returns:\n","            SVM scores of shape (batch_size,)\n","        \"\"\"\n","        # YOUR CODE HERE\n","        # Compute and return the raw scores\n","        # use .squeeze(1) after applying the linear layer to remove extra dimension\n","        pass\n","\n","    def predict(self, x):\n","        \"\"\"\n","        Predict class labels.\n","\n","        Args:\n","            x: features of shape (batch_size, input_dim)\n","\n","        Returns:\n","            Predicted class labels (-1 or 1) of shape (batch_size,)\n","        \"\"\"\n","        pass\n","        # apply forward, then...\n","        # HINT: use torch.where to convert all scores <= 0 to torch.tensor(-1.0), and otherwise to torch.tensor(1.0)\n","        # see documentation for torch.where: https://pytorch.org/docs/stable/generated/torch.where.html"]},{"cell_type":"markdown","id":"d5618885","metadata":{"deletable":true,"editable":true,"id":"d5618885"},"source":["### Exercise 1.2: Implement Hinge Loss\n","\n","Now we need to implement the hinge loss function for SVM training:\n","\n","$$L(y, \\hat{y}) = \\max(0, 1 - y \\cdot \\hat{y})$$\n","\n","where $y$ is the true label (-1 or 1) and $\\hat{y}$ is the predicted score.\n"]},{"cell_type":"code","execution_count":null,"id":"f779ed47","metadata":{"deletable":true,"editable":true,"id":"f779ed47","lines_to_next_cell":1},"outputs":[],"source":["# Exercise 1.2: Implement Hinge Loss for SVM\n","class HingeLoss(nn.Module):\n","    \"\"\"\n","    Hinge loss for binary SVM.\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(HingeLoss, self).__init__()\n","\n","    def forward(self, predictions, targets):\n","        \"\"\"\n","        Compute hinge loss.\n","\n","        Args:\n","            predictions: Raw SVM scores\n","            targets: True labels (-1 or 1)\n","\n","        Returns:\n","            Mean hinge loss\n","        \"\"\"\n","        # YOUR CODE HERE\n","        # Apply max(0, 1 - y * f(x)) at each position and take the mean of the result\n","        # HINT: rather than using torch.max (which gives maximum value across entire matrix)...\n","        # use torch.clamp to set the minimum of values to be 0 - look up documentation here: https://pytorch.org/docs/stable/generated/torch.clamp.html\n","        pass"]},{"cell_type":"markdown","id":"ef0a9f85","metadata":{"deletable":true,"editable":true,"id":"ef0a9f85"},"source":["### Exercise 1.3: Train Binary SVM\n","\n","Now let's train our SVM using the UniversalProcedure class and evaluate its performance with cross-validation.\n"]},{"cell_type":"code","execution_count":null,"id":"2c9745b4","metadata":{"deletable":true,"editable":true,"id":"2c9745b4"},"outputs":[],"source":["# Exercise 1.3: Train Binary SVM\n","\n","# get binary data\n","variation_level = \"V3\"\n","features_tensor, labels_tensor = prepare_binary_data(cat1, cat2, variation_level)\n","\n","# YOUR CODE HERE\n","# 1. Initialize the SVM model with input_dim (HINT: get those from the shape of the features_tensor)\n","# 2. Create a cross-validator instance\n","# 3. Create a UniversalProcedure instance with the HingeLoss as the loss_func\n","# 4. Train and evaluate the model using the evaluate method - use train_epochs=200, lr=0.001\n","\n","# Print results - uncomment below\n","# print(f\"SVM performance for {cat1.decode() if isinstance(cat1, bytes) else cat1} vs \"\n","#       f\"{cat2.decode() if isinstance(cat2, bytes) else cat2} at variation level {variation_level}:\")\n","# for metric, value in svm_results.items():\n","#     if 'CV' in metric and 'Std' not in metric:\n","#         print(f\"{metric}: {value:.4f}\")"]},{"cell_type":"markdown","id":"ead8f7ea","metadata":{"deletable":true,"editable":true,"id":"ead8f7ea"},"source":["### Visualize Decision Boundary\n","\n","To better understand our SVM classifier, let's visualize the decision boundary in 2D space.\n"]},{"cell_type":"code","execution_count":null,"id":"b1856e77","metadata":{"deletable":true,"editable":true,"id":"b1856e77","lines_to_next_cell":1},"outputs":[],"source":["def plot_svm_decision_boundary(vl, cat1, cat2):\n","    \"\"\"Plot data points and SVM decision boundary in 2D projection.\"\"\"\n","    cat1 = ensure_bytes(cat1)\n","    cat2 = ensure_bytes(cat2)\n","    vl = ensure_bytes(vl)\n","\n","    # Get the subset of data for the two categories\n","    subset_inds = (var_level == vl) & ((categories == cat1) | (categories == cat2))\n","    Neural_Data_subset = IT_Neural_Data[subset_inds]\n","    category_subset = categories[subset_inds]\n","\n","    # Project to 2D using MDS\n","    distmat = 1 - np.corrcoef(Neural_Data_subset)\n","    coords = MDS(dissimilarity=\"precomputed\", random_state=42).fit_transform(distmat)\n","    x, y = coords.T\n","\n","    # Plot data points\n","    plt.figure(figsize=(10, 8))\n","    cat1_label = cat1.decode() if isinstance(cat1, bytes) else cat1\n","    cat2_label = cat2.decode() if isinstance(cat2, bytes) else cat2\n","    plt.scatter(\n","        x[category_subset == cat1],\n","        y[category_subset == cat1],\n","        color=\"b\",\n","        label=cat1_label,\n","    )\n","    plt.scatter(\n","        x[category_subset == cat2],\n","        y[category_subset == cat2],\n","        color=\"r\",\n","        label=cat2_label,\n","    )\n","\n","    # Create binary labels (1 for cat1, -1 for cat2)\n","    binary_labels = torch.ones(len(category_subset))\n","    binary_labels[category_subset == cat2] = -1\n","\n","    # Create and train a 2D SVM model\n","    model_2d = BinaryLinearSVM(2)\n","    data_2d = torch.tensor(coords, dtype=torch.float32)\n","\n","    # Train with hinge loss\n","    optimizer = optim.Adam(model_2d.parameters(), lr=0.01)\n","    loss_fn = HingeLoss()\n","\n","    for _ in range(300):\n","        predictions = model_2d(data_2d)\n","        loss = loss_fn(predictions, binary_labels)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Create a mesh grid for decision boundary visualization\n","    x_min, x_max = x.min() - 0.5, x.max() + 0.5\n","    y_min, y_max = y.min() - 0.5, y.max() + 0.5\n","    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n","\n","    # Predict on the grid points\n","    grid_points = np.c_[xx.ravel(), yy.ravel()]\n","    with torch.no_grad():\n","        Z = model_2d(torch.tensor(grid_points, dtype=torch.float32)).numpy()\n","        Z = Z.reshape(xx.shape)\n","\n","    # Plot the decision boundary\n","    plt.contour(xx, yy, Z, levels=[0], colors=\"k\", linestyles=\"-\")\n","    plt.contourf(xx, yy, Z, levels=[-1, 0, 1], cmap=plt.cm.RdBu, alpha=0.3)\n","\n","    plt.xlabel(\"Component 1\", fontsize=14)\n","    plt.ylabel(\"Component 2\", fontsize=14)\n","    plt.title(\n","        f\"SVM Decision Boundary: {cat1_label} vs {cat2_label}, Variation Level {vl.decode() if isinstance(vl, bytes) else vl}\",\n","        fontsize=14,\n","    )\n","    plt.legend()\n","    plt.axis(\"equal\")\n","    plt.show()\n","\n","\n","variation_level = \"V3\"\n","plot_svm_decision_boundary(variation_level, cat1, cat2)"]},{"cell_type":"markdown","id":"072fa2f3","metadata":{"deletable":true,"editable":true,"id":"072fa2f3"},"source":["## Exercise 2: Multi-Class Classification with SVM\n","\n","Now let's extend our approach to multi-class classification. There are several strategies for multi-class SVMs, but we'll use the one-vs-all approach, where we train one SVM for each class against all others.\n"]},{"cell_type":"code","execution_count":null,"id":"70d7e0e3","metadata":{"deletable":true,"editable":true,"id":"70d7e0e3","lines_to_next_cell":1},"outputs":[],"source":["# Prepare data for multi-class classification\n","def prepare_multiclass_data(vl):\n","    \"\"\"\n","    Prepare data for multi-class classification.\n","\n","    Args:\n","        vl: Variation level\n","\n","    Returns:\n","        features_tensor: PyTorch tensor of neural data\n","        labels_tensor: PyTorch tensor of integer labels\n","        label_map: Dictionary mapping category names to integer indices\n","    \"\"\"\n","    vl = ensure_bytes(vl)\n","\n","    # Get indices for the specified variation level\n","    mask = var_level == vl\n","\n","    # Extract features\n","    features = IT_Neural_Data[mask]\n","    category_subset = categories[mask]\n","\n","    # Create integer labels and label mapping\n","    label_map = {cat: i for i, cat in enumerate(unique_categories)}\n","    labels = np.array([label_map[cat] for cat in category_subset])\n","\n","    # Convert to PyTorch tensors\n","    features_tensor = torch.tensor(features, dtype=torch.float32)\n","    labels_tensor = torch.tensor(labels, dtype=torch.long)  # Use long for class indices\n","\n","    return features_tensor, labels_tensor, label_map\n","\n","\n","# Prepare multi-class data at moderate variation\n","vl = \"V3\"\n","features_tensor, labels_tensor, label_map = prepare_multiclass_data(vl)\n","print(\n","    f\"Multi-class data: {features_tensor.shape[0]} samples, {features_tensor.shape[1]} features\"\n",")\n","print(f\"Number of classes: {len(unique_categories)}\")\n","\n","# Check class distribution\n","for cat, idx in label_map.items():\n","    cat_name = cat.decode() if isinstance(cat, bytes) else cat\n","    count = torch.sum(labels_tensor == idx).item()\n","    print(f\"Class {idx} ({cat_name}): {count} samples\")"]},{"cell_type":"markdown","id":"ec4d261a","metadata":{"deletable":true,"editable":true,"id":"ec4d261a"},"source":["### Exercise 2.1: Implement Multi-Class SVM\n","\n","Now, let's implement a multi-class SVM using the one-vs-all approach. We'll use a single linear layer that outputs a score for each class.\n"]},{"cell_type":"code","execution_count":null,"id":"c104ca73","metadata":{"deletable":true,"editable":true,"id":"c104ca73","lines_to_next_cell":1},"outputs":[],"source":["# Exercise 2.1: Implement Multi-Class SVM\n","class MultiClassSVM(nn.Module):\n","    \"\"\"\n","    Multi-class SVM using one-vs-all approach.\n","    \"\"\"\n","\n","    def __init__(self, input_dim, num_classes):\n","        \"\"\"\n","        Initialize the multi-class SVM model.\n","\n","        Args:\n","            input_dim: Dimensionality of input features\n","            num_classes: Number of classes to classify\n","        \"\"\"\n","        super(MultiClassSVM, self).__init__()\n","\n","        # YOUR CODE HERE\n","        # Define the model parameters using nn.Linear\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass to compute scores for each class.\n","\n","        Args:\n","            x: Input features of shape (batch_size, input_dim)\n","\n","        Returns:\n","            Class scores of shape (batch_size, num_classes)\n","        \"\"\"\n","        # YOUR CODE HERE\n","        # Compute and return scores for each class\n","        # HINT: this should be implemented the same as the binary class, but this time no need to squeeze!\n","        pass\n","\n","    def predict(self, x):\n","        \"\"\"\n","        Predict class labels.\n","\n","        Args:\n","            x: Input features of shape (batch_size, input_dim)\n","\n","        Returns:\n","            Predicted class labels of shape (batch_size,)\n","        \"\"\"\n","        # YOUR CODE HERE (HINT:  apply forward, then use torch.argmax: https://pytorch.org/docs/stable/generated/torch.argmax.html)\n","        pass"]},{"cell_type":"markdown","id":"444ccd8d","metadata":{"deletable":true,"editable":true,"id":"444ccd8d"},"source":["### Multi-Class Hinge Loss\n","\n","For multi-class SVMs, we need a modified hinge loss that considers scores for all classes. The goal is to make the score for the correct class higher than scores for all other classes by at least a margin of 1. Let's review how this is implemented below:\n"]},{"cell_type":"code","execution_count":null,"id":"92668407","metadata":{"deletable":true,"editable":true,"id":"92668407","lines_to_next_cell":1},"outputs":[],"source":["class MultiClassHingeLoss(nn.Module):\n","    \"\"\"\n","    Multi-class hinge loss for SVM.\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(MultiClassHingeLoss, self).__init__()\n","\n","    def forward(self, predictions, targets):\n","        \"\"\"\n","        Compute multi-class hinge loss.\n","\n","        Args:\n","            predictions: Class scores of shape (batch_size, num_classes)\n","            targets: Class indices of shape (batch_size,)\n","\n","        Returns:\n","            Mean multi-class hinge loss\n","        \"\"\"\n","        batch_size = targets.size(0)\n","\n","        # Get the score for the correct class for each sample\n","        correct_class_scores = predictions[torch.arange(batch_size), targets]\n","\n","        # Calculate margins: score_j - score_yi + 1\n","        # We add 1 to make sure correct class score is higher by at least 1\n","        margins = predictions - correct_class_scores.unsqueeze(1) + 1.0\n","\n","        # Don't include the margin for the correct class (should be 1)\n","        margins[torch.arange(batch_size), targets] = 0\n","\n","        # Clamp at 0 (hinge) and sum over classes\n","        loss = torch.clamp(margins, min=0).sum(dim=1) / (predictions.size(1) - 1)\n","\n","        # Return mean loss over the batch\n","        return loss.mean()"]},{"cell_type":"markdown","id":"534624de","metadata":{"deletable":true,"editable":true,"id":"534624de"},"source":["### Exercise 2.3: Train Multi-Class SVM\n","\n","Now let's train our multi-class SVM using the Universal Procedure class.\n"]},{"cell_type":"code","execution_count":null,"id":"526ce37b","metadata":{"deletable":true,"editable":true,"id":"526ce37b"},"outputs":[],"source":["# Exercise 2.3: Train Multi-Class SVM\n","\n","# Prepare multi-class data at moderate variation\n","vl = \"V3\"\n","features_tensor, labels_tensor, label_map = prepare_multiclass_data(vl)\n","\n","# YOUR CODE HERE\n","# 1. Initialize the multi-class SVM model - HINT: get input_dim from the shape of the features_tensor, and use len(unique_categories) to get num_classes\n","# 2. Create a UniversalProcedure instance with the multi-class hinge loss\n","# 3. Train and evaluate the model - use train_epochs = 200, lr=0.01\n","\n","# Print the cross-validation accuracy - uncomment below"]},{"cell_type":"markdown","id":"9232f007","metadata":{"deletable":true,"editable":true,"id":"9232f007"},"source":["## Exercise 3: Logistic Regression\n","\n","Now we'll implement logistic regression with softmax, which provides a probabilistic approach to classification. Unlike SVM which focuses on maximizing margins, logistic regression models the probability distribution over classes.\n","\n","The key differences between SVM and logistic regression are:\n","\n","1. **Output Interpretation**:\n","   - SVM: Raw scores that determine class boundaries\n","   - Logistic Regression: Probabilities that sum to 1 (via softmax)\n","2. **Loss Function**:\n","   - SVM: Hinge loss that cares about margin violations\n","   - Logistic Regression: Cross-entropy loss that maximizes the likelihood of correct classes\n","3. **Decision Boundary**:\n","   - SVM: Maximizes the margin between closest examples\n","   - Logistic Regression: Places boundary where posterior probabilities equal 0.5\n"]},{"cell_type":"code","execution_count":null,"id":"2effb071","metadata":{"deletable":true,"editable":true,"id":"2effb071","lines_to_next_cell":1},"outputs":[],"source":["# Exercise 3.1: Implement the Softmax function\n","def softmax(x):\n","    \"\"\"\n","    Compute softmax values for each set of scores in x.\n","\n","    Args:\n","        x: Input tensor of shape (batch_size, num_classes)\n","\n","    Returns:\n","        Softmax probabilities of shape (batch_size, num_classes)\n","    \"\"\"\n","    # YOUR CODE HERE\n","    # compute exp(x) / sum of exp(x) over the 1st dimension\n","    # use keepdim=True in the torch.sum function\n","    pass"]},{"cell_type":"code","execution_count":null,"id":"2366b460","metadata":{"deletable":true,"editable":true,"id":"2366b460","lines_to_next_cell":1},"outputs":[],"source":["# Exercise 3.2: Implement Logistic Regression with Softmax\n","class LogisticRegression(nn.Module):\n","    \"\"\"\n","    Multi-class logistic regression model with softmax.\n","    \"\"\"\n","\n","    def __init__(self, input_dim, num_classes):\n","        \"\"\"\n","        Initialize the logistic regression model.\n","\n","        Args:\n","            input_dim: Dimensionality of input features\n","            num_classes: Number of classes to classify\n","        \"\"\"\n","        super(LogisticRegression, self).__init__()\n","\n","        # YOUR CODE HERE\n","        # Define the model parameters using nn.Linear\n","        self.linear = nn.Linear(input_dim, num_classes)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass to compute class probabilities.\n","\n","        Args:\n","            x: Input features of shape (batch_size, input_dim)\n","\n","        Returns:\n","            Class probabilities of shape (batch_size, num_classes)\n","        \"\"\"\n","        # YOUR CODE HERE\n","        # Apply linear transformation and then softmax\n","        pass\n","\n","    def predict(self, x):\n","        \"\"\"\n","        Predict class labels.\n","\n","        Args:\n","            x: Input features of shape (batch_size, input_dim)\n","\n","        Returns:\n","            Predicted class labels of shape (batch_size,)\n","        \"\"\"\n","        # YOUR CODE HERE\n","        pass"]},{"cell_type":"markdown","id":"rwYqYV_NbGXs","metadata":{"id":"rwYqYV_NbGXs"},"source":["### Exercise 3.3: Train a Multi-Class Logistic Regression Model\n"]},{"cell_type":"markdown","source":["#### Note: Cross-Entropy Loss for Logistic Regression\n","\n","For logistic regression, we use cross-entropy loss, which measures the difference between two probability distributions: the predicted probabilities and the true distribution (one-hot encoded target).\n","\n","The cross-entropy loss is defined as:\n","\n","$$L(y, \\hat{y}) = -\\sum_{i=1}^{n} y_i \\log(p(\\hat{y})_i)$$\n","\n","where:\n","\n","- $y_i$ is the true probability of class $i$ (typically 0 or 1 for one-hot encoding)\n","- $\\hat{y}_i$ is the predicted probability of class $i$\n","\n","For multi-class classification with one-hot encoded targets (meaning the true class probability becomes 1), if $c$ is the correct class index, this simplifies to:\n","\n","$$L(y, \\hat{y}) = -\\log(\\hat{y}_c)$$\n","\n","PyTorch provides the cross-entropy loss as a built-in function: `torch.nn.CrossEntropyLoss()`. This function combines softmax and negative log-likelihood loss in a single, more numerically stable operation.\n","\n","This means we don't even need to apply softmax in our logistic regression function since it is applied inside the loss function. That is, it's enough to return what is called the logits, or unnormalized log probabilities (this are the scores you got after applying the linear layer and before you applied softmax in the forward function). However, if you want output probabilities, you would still need to apply softmax.\n","\n","\n","The torch loss function that takes in log probabilities directly - meaning it does not apply softmax automatically, is called `torch.nn.NLLLoss()`, and that is what we will use since you already got the probabilities via softmax.\n"],"metadata":{"id":"FZS0rdR5ahTS"},"id":"FZS0rdR5ahTS"},{"cell_type":"code","execution_count":null,"id":"a6f1c163","metadata":{"deletable":true,"editable":true,"id":"a6f1c163"},"outputs":[],"source":["nll_loss = lambda probs, target: nn.NLLLoss()(torch.log(probs), target)\n","\n","# YOUR CODE HERE\n","# 1. Use the same prepared multi-class data from the SVM case\n","# 2. Initialize the LogisticRegression model\n","# 3. Create a UniversalProcedure instance with nll_loss as its loss function\n","# 4. Train and evaluate the model\n","\n","# Print results - uncomment below\n","# print(f\"Logistic Regression performance at variation level {vl}:\")\n","# for metric, value in logistic_results.items():\n","#     if 'CV' in metric and 'Std' not in metric:\n","#         print(f\"{metric}: {value:.4f}\")\n","\n","# # Compare results between SVM and Logistic Regression\n","# print(\"\\nComparison between SVM and Logistic Regression:\")\n","# print(f\"SVM Accuracy: {svm_results['CV Accuracy']:.4f}\")\n","# print(f\"Logistic Regression Accuracy: {logistic_results['CV Accuracy']:.4f}\")"]},{"cell_type":"markdown","id":"ef8c3a64","metadata":{"deletable":true,"editable":true,"id":"ef8c3a64"},"source":["### Classifier Performance Across Variation Levels\n","\n","Now let's evaluate how our classifiers perform across different variation levels. We expect that as variation increases, classification performance will decrease, since the neural representations become more variable.\n"]},{"cell_type":"code","execution_count":null,"id":"798a6e86","metadata":{"deletable":true,"editable":true,"id":"798a6e86","lines_to_next_cell":1},"outputs":[],"source":["def train_and_evaluate_across_variations(model_class, loss_func):\n","    \"\"\"\n","    Train and evaluate a model across different variation levels.\n","\n","    Args:\n","        model_class: The classifier class (MultiClassSVM or LogisticRegression)\n","        loss_func: The loss function to use\n","\n","    Returns:\n","        Dictionary mapping variation levels to accuracy scores\n","    \"\"\"\n","    results = {}\n","\n","    # Get all variation levels\n","    variation_levels = np.unique(var_level)\n","\n","    # Train and evaluate for each variation level\n","    for vl in variation_levels:\n","        # Prepare data for this variation level\n","        features_tensor, labels_tensor, label_map = prepare_multiclass_data(vl)\n","\n","        # Initialize the model\n","        input_dim = features_tensor.shape[1]\n","        num_classes = len(unique_categories)\n","        model = model_class(input_dim, num_classes)\n","\n","        # Create cross-validation splitter\n","        cv_splitter = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n","\n","        # Create UniversalProcedure with appropriate loss\n","        procedure = UniversalProcedure(\n","            cross_validator=cv_splitter,\n","            loss_func=loss_func,\n","        )\n","\n","        # Train and evaluate\n","        vl_results, _ = procedure.evaluate(\n","            model=model, x=features_tensor, y=labels_tensor, train_epochs=200, lr=0.01\n","        )\n","\n","        # Store accuracy for this variation level\n","        vl_decoded = vl.decode() if isinstance(vl, bytes) else vl\n","        results[vl_decoded] = vl_results[\"CV Accuracy\"]\n","\n","        # Print progress\n","        print(f\"Completed {model_class.__name__} for variation level {vl_decoded}\")\n","\n","    return results"]},{"cell_type":"code","execution_count":null,"id":"fd368479","metadata":{"deletable":true,"editable":true,"id":"fd368479"},"outputs":[],"source":["# Train models across variation levels\n","print(\"Training SVM across variation levels...\")\n","svm_var_results = train_and_evaluate_across_variations(\n","    MultiClassSVM, MultiClassHingeLoss()\n",")\n","\n","print(\"\\nTraining Logistic Regression across variation levels...\")\n","log_var_results = train_and_evaluate_across_variations(\n","    LogisticRegression, nn.CrossEntropyLoss()\n",")"]},{"cell_type":"code","execution_count":null,"id":"c7dd85b9","metadata":{"deletable":true,"editable":true,"id":"c7dd85b9"},"outputs":[],"source":["# Plot performance across variation levels\n","plt.figure(figsize=(10, 6))\n","variation_levels = sorted(svm_var_results.keys())\n","svm_accuracies = [svm_var_results[vl] for vl in variation_levels]\n","log_accuracies = [log_var_results[vl] for vl in variation_levels]\n","\n","plt.plot(variation_levels, svm_accuracies, \"bo-\", label=\"SVM\")\n","plt.plot(variation_levels, log_accuracies, \"ro-\", label=\"Logistic Regression\")\n","plt.xlabel(\"Variation Level\", fontsize=14)\n","plt.ylabel(\"Accuracy\", fontsize=14)\n","plt.title(\"Classifier Performance Across Variation Levels\", fontsize=16)\n","plt.xticks(rotation=45)\n","plt.legend(fontsize=12)\n","plt.grid(True, alpha=0.3)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","id":"2f01a5b3","metadata":{"deletable":true,"editable":true,"id":"2f01a5b3"},"source":["## Discussion Questions\n","\n","1. **How does performance change with variation level?** What does this tell us about the neural representation in IT cortex?\n","\n","2. **Comparing SVM and Logistic Regression**: Which model performs better?\n","\n","3. **Biological plausibility**: SVM and logistic regression are both linear classifiers. Do you think these models resemble how the brain might decode visual information? Why or why not?\n"]},{"cell_type":"markdown","id":"a71fd2b8","metadata":{"deletable":true,"editable":true,"id":"a71fd2b8"},"source":["## Conclusion\n","\n","In this notebook, we explored binary and multi-class classification using PyTorch:\n","\n","1. **Support Vector Machines (SVMs)**:\n","\n","   - Implemented binary SVM with hinge loss\n","   - Extended to multi-class classification using one-vs-all approach\n","   - Used margin-based optimization to find the decision boundary\n","\n","2. **Logistic Regression**:\n","\n","   - Implemented softmax function for probability outputs\n","   - Used cross-entropy loss to train the model\n","   - Compared performance with SVM\n","\n","3. **Effect of Variation**:\n","   - Evaluated how classification performance changes with increasing stimulus variation\n","   - Visualized the neural representations in 2D space\n","\n","Key takeaways:\n","\n","- Both SVM and logistic regression are effective linear classifiers\n","- While they share a similar linear architecture (both use nn.Linear), they differ in their loss functions and output interpretations\n","- SVM maximizes margin, while logistic regression outputs probabilities\n","- The UniversalProcedure class provides a consistent way to train and evaluate multiple models\n","- Classification performance decreases with increasing variation level, suggesting that neural representations become more overlapping\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":5}